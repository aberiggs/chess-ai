{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import chess\n",
    "import chess.pgn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = \"lichess_2017-10_dataset.pth\"\n",
    "validation_dataset = \"lichess_2017-08_dataset.pth\"\n",
    "model_path = \"chess_model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_tensor(board, color):\n",
    "    \"\"\"\n",
    "    Convert a chess board to a tensor\n",
    "    \"\"\"\n",
    "    # Initialize tensor\n",
    "    tensor = torch.zeros(15, 8, 8)\n",
    "    \n",
    "    # Fill tensor with pieces\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            piece = board.piece_at(chess.square(i, j))\n",
    "            if piece is not None:\n",
    "                piece_type = piece.piece_type\n",
    "                piece_color = piece.color\n",
    "                tensor[piece_type + 6 * (piece_color == color), i, j] = 1\n",
    "    \n",
    "    # Fill tensor with en passant squares\n",
    "    if board.ep_square is not None:\n",
    "        tensor[12, board.ep_square // 8, board.ep_square % 8] = 1\n",
    "    \n",
    "    # Fill tensor with castling rights\n",
    "    white_castle_channel = 13 if color is chess.WHITE else 14\n",
    "    if board.has_kingside_castling_rights(chess.WHITE):\n",
    "        tensor[white_castle_channel, 6, 0] = 1\n",
    "    if board.has_queenside_castling_rights(chess.WHITE):\n",
    "        tensor[white_castle_channel, 2, 0] = 1\n",
    "    if board.has_kingside_castling_rights(chess.BLACK):\n",
    "        tensor[27 - white_castle_channel, 6, 7] = 1\n",
    "    if board.has_queenside_castling_rights(chess.BLACK):\n",
    "        tensor[27 - white_castle_channel, 2, 7] = 1\n",
    "    \n",
    "    return tensor\n",
    "\n",
    "def move_to_index(move) -> int:\n",
    "    \"\"\"\n",
    "    Convert a move to an index\n",
    "    \"\"\"\n",
    "    # Get the coordinates of the move\n",
    "    from_square = move.from_square\n",
    "    to_square = move.to_square\n",
    "    \n",
    "    # Convert the coordinates to the output tensor\n",
    "    from_x, from_y = from_square // 8, from_square % 8\n",
    "    to_x, to_y = to_square // 8, to_square % 8\n",
    "\n",
    "    assert from_x >= 0 and from_x < 8\n",
    "    assert from_y >= 0 and from_y < 8\n",
    "    assert to_x >= 0 and to_x < 8\n",
    "    assert to_y >= 0 and to_y < 8\n",
    "\n",
    "    # 0bjjjkkklllmmm : j=from_x, k=from_y, l=to_x, m=to_y\n",
    "    return (from_x << 9) | (from_y << 6) | (to_x << 3) | (to_y)\n",
    "    # print(\"from_x\", from_x, \"from_y\", from_y, \"to_x\", to_x, \"to_y\", to_y, \"index\", index)\n",
    "    \n",
    "\n",
    "def move_to_output_tensor(move) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a move to the coordinate in the output tensor (8x8x8x8)\n",
    "    \"\"\"\n",
    "\n",
    "    # output_tensor = torch.zeros(8*8*8*8)\n",
    "    # output_tensor[move_to_index(move)] = 1\n",
    "    return torch.tensor(move_to_index(move))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.fens = []\n",
    "        self.next_moves = []\n",
    "        for sample in data:\n",
    "            self.fens.append(sample['fen'])\n",
    "            self.next_moves.append(sample['next_move'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        board = chess.Board(self.fens[idx])\n",
    "        color = board.turn\n",
    "        board_tensor = board_to_tensor(board, color)\n",
    "        next_move = move_to_output_tensor(self.next_moves[idx])\n",
    "\n",
    "        sample = {'board': board_tensor, 'next_move': next_move}\n",
    "        return sample\n",
    "    \n",
    "class ChessValueDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.fens = []\n",
    "        self.results = []\n",
    "        for sample in data:\n",
    "            self.fens.append(sample['fen'])\n",
    "            self.results.append(sample['result'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        board = chess.Board(self.fens[idx])\n",
    "        board_tensor = board_to_tensor(board, chess.WHITE)\n",
    "        result = torch.tensor(self.results[idx])\n",
    "\n",
    "        sample = {'board': board_tensor, 'result': result}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m compressed_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlichess_2017-10.pgn.zst\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     90\u001b[0m pgn_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.pgn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 91\u001b[0m \u001b[43mdecompress_zstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.pgn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m dst_file \u001b[38;5;241m=\u001b[39m compressed_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_value_dataset.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m generate_value_dataset(pgn_file, dst_file)\n",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m, in \u001b[0;36mdecompress_zstd\u001b[0;34m(in_file, out_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m decomp \u001b[38;5;241m=\u001b[39m zstd\u001b[38;5;241m.\u001b[39mZstdDecompressor()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m decompressed_file:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mdecomp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompressed_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecompressed_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import zstandard as zstd\n",
    "\n",
    "def decompress_zstd(in_file, out_file):\n",
    "    with open(in_file, 'rb') as compressed_file:\n",
    "        decomp = zstd.ZstdDecompressor()\n",
    "        with open(out_file, 'wb') as decompressed_file:\n",
    "            decomp.copy_stream(compressed_file, decompressed_file)\n",
    "\n",
    "def extract_data(game) -> list:\n",
    "    data = []\n",
    "    board = game.board()\n",
    "    game_result = game.headers['Result'][0]\n",
    "    if game_result == '1':\n",
    "        winning_color = chess.WHITE\n",
    "    else:\n",
    "        winning_color = chess.BLACK\n",
    "    \n",
    "    for move in game.mainline_moves():\n",
    "        if board.turn == winning_color:\n",
    "            data.append({'fen': board.fen(), 'next_move': move})\n",
    "        board.push(move)\n",
    "    return data\n",
    "\n",
    "def extract_value_data(game) -> list:\n",
    "    data = []\n",
    "    board = game.board()\n",
    "    game_result = game.headers['Result']\n",
    "    if game_result == '1-0':\n",
    "        result = 1\n",
    "    elif game_result == '0-1':\n",
    "        result = -1\n",
    "    else:\n",
    "        result = 0\n",
    "    \n",
    "    for move in game.mainline_moves():\n",
    "        data.append({'fen': board.fen(), 'result': result})\n",
    "        board.push(move)\n",
    "    return data\n",
    "\n",
    "def parse_pgn_file(file, max_games = 0) -> list:\n",
    "    data = []\n",
    "    tenths_done = -1\n",
    "    game_count = 0\n",
    "    extracted_game_count = 0\n",
    "    with open(file) as pgn_file:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(pgn_file)\n",
    "            if game is None: # No more games in file\n",
    "                break \n",
    "            game_count += 1\n",
    "            \n",
    "            white_elo = int(game.headers['WhiteElo'])\n",
    "            black_elo = int(game.headers['BlackElo'])\n",
    "            if white_elo > 2000 or black_elo > 2000:\n",
    "                data += extract_value_data(game)\n",
    "                extracted_game_count += 1\n",
    "            \n",
    "            if max_games > 0 and tenths_done < (game_count / max_games) * 10:\n",
    "                tenths_done += 1\n",
    "                print(\"[\", end=\"\")\n",
    "                print(\"=\" * tenths_done, end=\"\")\n",
    "                print(\".\" * (10 - tenths_done), end=\"\")\n",
    "                print(\"]\")\n",
    "                \n",
    "            if game_count == max_games:\n",
    "                break\n",
    "\n",
    "    print(f\"Extracted {len(data)} moves from {extracted_game_count} games\")\n",
    "    return data\n",
    "\n",
    "def generate_dataset(pgn_file: str, dst_file: str):\n",
    "    print(\"Parsing pgn: \", os.path.abspath(pgn_file))\n",
    "    data = parse_pgn_file(pgn_file, 100000)\n",
    "\n",
    "    dataset = ChessDataset(data)\n",
    "    print(\"Generated a dataset with\", len(dataset), \"samples\")\n",
    "    torch.save(dataset, dst_file)\n",
    "    print(\"Saved dataset to\", os.path.abspath(dst_file))\n",
    "    \n",
    "def generate_value_dataset(pgn_file: str, dst_file: str):\n",
    "    print(\"Parsing pgn: \", os.path.abspath(pgn_file))\n",
    "    data = parse_pgn_file(pgn_file, 2000000)\n",
    "\n",
    "    dataset = ChessValueDataset(data)\n",
    "    print(\"Generated a dataset with\", len(dataset), \"samples\")\n",
    "    torch.save(dataset, dst_file)\n",
    "    print(\"Saved dataset to\", os.path.abspath(dst_file))\n",
    "      \n",
    "compressed_file = \"lichess_2017-10.pgn.zst\" \n",
    "pgn_file = \"data.pgn\"\n",
    "decompress_zstd(compressed_file, \"data.pgn\")\n",
    "dst_file = compressed_file.split(\".\")[0] + \"_value_dataset.pth\"\n",
    "generate_value_dataset(pgn_file, dst_file)\n",
    "# remove the pgn file\n",
    "os.remove(pgn_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessModel, self).__init__()\n",
    "        self.conv_nn_stack = nn.Sequential(\n",
    "            # Convolutional layers\n",
    "            nn.Conv2d(15, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            # Flatten the tensor for the fully connected layers\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Fully connected layers\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(1024, 8*8*8*8)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.conv_nn_stack(x)\n",
    "        return logits\n",
    "    \n",
    "class ChessValueModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessValueModel, self).__init__()\n",
    "        self.conv_nn_stack = nn.Sequential(\n",
    "            # Convolutional layers\n",
    "            nn.Conv2d(15, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            # Flatten the tensor for the fully connected layers\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Fully connected layers\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(1024, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.conv_nn_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to train on 7854493 samples\n",
      "DataLoader ready with 122727 batches\n",
      "Performing training using cuda\n",
      "Model will be saved to /home/azriv/chess-ai/notebooks/chess_model.pth\n",
      "----- STARTING EPOCH 1 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1381.2153959274292\n",
      "[Batch #1000/122727] Loss: 1375.5920605659485\n",
      "[Batch #1500/122727] Loss: 1382.9891641139984\n",
      "[Batch #2000/122727] Loss: 1378.6258471012115\n",
      "[Batch #2500/122727] Loss: 1380.3182408809662\n",
      "[Batch #3000/122727] Loss: 1373.0340321063995\n",
      "[Batch #3500/122727] Loss: 1367.0477449893951\n",
      "[Batch #4000/122727] Loss: 1368.3066864013672\n",
      "[Batch #4500/122727] Loss: 1374.262111902237\n",
      "[Batch #5000/122727] Loss: 1375.5909633636475\n",
      "[Batch #5500/122727] Loss: 1370.9266211986542\n",
      "[Batch #6000/122727] Loss: 1375.1286079883575\n",
      "[Batch #6500/122727] Loss: 1359.4769432544708\n",
      "[Batch #7000/122727] Loss: 1372.1156668663025\n",
      "[Batch #7500/122727] Loss: 1370.0122697353363\n",
      "[Batch #8000/122727] Loss: 1366.6716086864471\n",
      "[Batch #8500/122727] Loss: 1371.9124805927277\n",
      "[Batch #9000/122727] Loss: 1372.2289083003998\n",
      "[Batch #9500/122727] Loss: 1368.2379808425903\n",
      "[Batch #10000/122727] Loss: 1364.3960845470428\n",
      "[Batch #10500/122727] Loss: 1374.7328555583954\n",
      "[Batch #11000/122727] Loss: 1367.3981366157532\n",
      "[Batch #11500/122727] Loss: 1367.4838066101074\n",
      "[Batch #12000/122727] Loss: 1373.5906519889832\n",
      "[Batch #12500/122727] Loss: 1360.1308023929596\n",
      "[Batch #13000/122727] Loss: 1376.887746810913\n",
      "[Batch #13500/122727] Loss: 1367.0962715148926\n",
      "[Batch #14000/122727] Loss: 1366.3705995082855\n",
      "[Batch #14500/122727] Loss: 1378.461142539978\n",
      "[Batch #15000/122727] Loss: 1378.2272198200226\n",
      "[Batch #15500/122727] Loss: 1365.2102031707764\n",
      "[Batch #16000/122727] Loss: 1364.9544014930725\n",
      "[Batch #16500/122727] Loss: 1372.1211190223694\n",
      "[Batch #17000/122727] Loss: 1365.8171138763428\n",
      "[Batch #17500/122727] Loss: 1369.1847438812256\n",
      "[Batch #18000/122727] Loss: 1375.9495952129364\n",
      "[Batch #18500/122727] Loss: 1370.4304203987122\n",
      "[Batch #19000/122727] Loss: 1380.8760731220245\n",
      "[Batch #19500/122727] Loss: 1370.0714446306229\n",
      "[Batch #20000/122727] Loss: 1365.4828045368195\n",
      "[Batch #20500/122727] Loss: 1369.085607767105\n",
      "[Batch #21000/122727] Loss: 1367.8035824298859\n",
      "[Batch #21500/122727] Loss: 1372.4662356376648\n",
      "[Batch #22000/122727] Loss: 1372.9841570854187\n",
      "[Batch #22500/122727] Loss: 1365.9177244901657\n",
      "[Batch #23000/122727] Loss: 1372.1958634853363\n",
      "[Batch #23500/122727] Loss: 1368.7566695213318\n",
      "[Batch #24000/122727] Loss: 1366.381070137024\n",
      "[Batch #24500/122727] Loss: 1368.6905839443207\n",
      "[Batch #25000/122727] Loss: 1363.6553704738617\n",
      "[Batch #25500/122727] Loss: 1366.3206973075867\n",
      "[Batch #26000/122727] Loss: 1367.6471939086914\n",
      "[Batch #26500/122727] Loss: 1355.5921568870544\n",
      "[Batch #27000/122727] Loss: 1359.295363664627\n",
      "[Batch #27500/122727] Loss: 1370.2739872932434\n",
      "[Batch #28000/122727] Loss: 1367.1710586547852\n",
      "[Batch #28500/122727] Loss: 1371.6080176830292\n",
      "[Batch #29000/122727] Loss: 1368.785656929016\n",
      "[Batch #29500/122727] Loss: 1364.119032740593\n",
      "[Batch #30000/122727] Loss: 1361.9573237895966\n",
      "[Batch #30500/122727] Loss: 1371.2319993972778\n",
      "[Batch #31000/122727] Loss: 1361.042919754982\n",
      "[Batch #31500/122727] Loss: 1355.1212980747223\n",
      "[Batch #32000/122727] Loss: 1366.639747619629\n",
      "[Batch #32500/122727] Loss: 1367.8822538852692\n",
      "[Batch #33000/122727] Loss: 1372.899240732193\n",
      "[Batch #33500/122727] Loss: 1367.0334305763245\n",
      "[Batch #34000/122727] Loss: 1362.6995539665222\n",
      "[Batch #34500/122727] Loss: 1363.8693072795868\n",
      "[Batch #35000/122727] Loss: 1375.1088678836823\n",
      "[Batch #35500/122727] Loss: 1373.1211268901825\n",
      "[Batch #36000/122727] Loss: 1375.219162940979\n",
      "[Batch #36500/122727] Loss: 1371.7433497905731\n",
      "[Batch #37000/122727] Loss: 1362.6051197052002\n",
      "[Batch #37500/122727] Loss: 1367.7932424545288\n",
      "[Batch #38000/122727] Loss: 1365.193832874298\n",
      "[Batch #38500/122727] Loss: 1371.6527428627014\n",
      "[Batch #39000/122727] Loss: 1367.3936564922333\n",
      "[Batch #39500/122727] Loss: 1369.1537301540375\n",
      "[Batch #40000/122727] Loss: 1368.1722288131714\n",
      "[Batch #40500/122727] Loss: 1364.3736445903778\n",
      "[Batch #41000/122727] Loss: 1363.9834296703339\n",
      "[Batch #41500/122727] Loss: 1365.8882794380188\n",
      "[Batch #42000/122727] Loss: 1363.0264239311218\n",
      "[Batch #42500/122727] Loss: 1371.8909537792206\n",
      "[Batch #43000/122727] Loss: 1365.8732271194458\n",
      "[Batch #43500/122727] Loss: 1365.0234117507935\n",
      "[Batch #44000/122727] Loss: 1364.5424304008484\n",
      "[Batch #44500/122727] Loss: 1375.3485748767853\n",
      "[Batch #45000/122727] Loss: 1361.4982888698578\n",
      "[Batch #45500/122727] Loss: 1370.2418248653412\n",
      "[Batch #46000/122727] Loss: 1359.8923468589783\n",
      "[Batch #46500/122727] Loss: 1363.1473860740662\n",
      "[Batch #47000/122727] Loss: 1361.46185028553\n",
      "[Batch #47500/122727] Loss: 1368.7384254932404\n",
      "[Batch #48000/122727] Loss: 1366.1316919326782\n",
      "[Batch #48500/122727] Loss: 1372.9499771595001\n",
      "[Batch #49000/122727] Loss: 1371.4020555019379\n",
      "[Batch #49500/122727] Loss: 1362.149840593338\n",
      "[Batch #50000/122727] Loss: 1364.3162469863892\n",
      "[Batch #50500/122727] Loss: 1373.1944992542267\n",
      "[Batch #51000/122727] Loss: 1370.6265864372253\n",
      "[Batch #51500/122727] Loss: 1372.6348812580109\n",
      "[Batch #52000/122727] Loss: 1375.4162311553955\n",
      "[Batch #52500/122727] Loss: 1374.0231161117554\n",
      "[Batch #53000/122727] Loss: 1371.5758442878723\n",
      "[Batch #53500/122727] Loss: 1367.8435453176498\n",
      "[Batch #54000/122727] Loss: 1371.3207898139954\n",
      "[Batch #54500/122727] Loss: 1377.5375291109085\n",
      "[Batch #55000/122727] Loss: 1366.2014780044556\n",
      "[Batch #55500/122727] Loss: 1368.1717720031738\n",
      "[Batch #56000/122727] Loss: 1359.0015852451324\n",
      "[Batch #56500/122727] Loss: 1368.918813943863\n",
      "[Batch #57000/122727] Loss: 1367.3897433280945\n",
      "[Batch #57500/122727] Loss: 1364.129530787468\n",
      "[Batch #58000/122727] Loss: 1366.7842309474945\n",
      "[Batch #58500/122727] Loss: 1368.3040630817413\n",
      "[Batch #59000/122727] Loss: 1374.916561126709\n",
      "[Batch #59500/122727] Loss: 1371.1833837032318\n",
      "[Batch #60000/122727] Loss: 1372.57071018219\n",
      "[Batch #60500/122727] Loss: 1373.5071601867676\n",
      "[Batch #61000/122727] Loss: 1372.768880367279\n",
      "[Batch #61500/122727] Loss: 1364.0479102134705\n",
      "[Batch #62000/122727] Loss: 1371.8246901035309\n",
      "[Batch #62500/122727] Loss: 1370.8764851093292\n",
      "[Batch #63000/122727] Loss: 1370.951506137848\n",
      "[Batch #63500/122727] Loss: 1361.3426005840302\n",
      "[Batch #64000/122727] Loss: 1367.3162803649902\n",
      "[Batch #64500/122727] Loss: 1370.3144927024841\n",
      "[Batch #65000/122727] Loss: 1364.2713809013367\n",
      "[Batch #65500/122727] Loss: 1363.3802201747894\n",
      "[Batch #66000/122727] Loss: 1370.7658350467682\n",
      "[Batch #66500/122727] Loss: 1362.757273197174\n",
      "[Batch #67000/122727] Loss: 1365.641741514206\n",
      "[Batch #67500/122727] Loss: 1374.1039853096008\n",
      "[Batch #68000/122727] Loss: 1366.9942071437836\n",
      "[Batch #68500/122727] Loss: 1366.2931778430939\n",
      "[Batch #69000/122727] Loss: 1357.0405402183533\n",
      "[Batch #69500/122727] Loss: 1372.1134128570557\n",
      "[Batch #70000/122727] Loss: 1367.6478004455566\n",
      "[Batch #70500/122727] Loss: 1373.0834193229675\n",
      "[Batch #71000/122727] Loss: 1375.0109057426453\n",
      "[Batch #71500/122727] Loss: 1370.1998524665833\n",
      "[Batch #72000/122727] Loss: 1367.2273795604706\n",
      "[Batch #72500/122727] Loss: 1371.663136959076\n",
      "[Batch #73000/122727] Loss: 1369.0482041835785\n",
      "[Batch #73500/122727] Loss: 1356.24373960495\n",
      "[Batch #74000/122727] Loss: 1360.9720911979675\n",
      "[Batch #74500/122727] Loss: 1363.4989318847656\n",
      "[Batch #75000/122727] Loss: 1373.4542924165726\n",
      "[Batch #75500/122727] Loss: 1373.6510291099548\n",
      "[Batch #76000/122727] Loss: 1365.1542601585388\n",
      "[Batch #76500/122727] Loss: 1368.7764418125153\n",
      "[Batch #77000/122727] Loss: 1364.6822321414948\n",
      "[Batch #77500/122727] Loss: 1371.2851054668427\n",
      "[Batch #78000/122727] Loss: 1365.9110028743744\n",
      "[Batch #78500/122727] Loss: 1368.9523632526398\n",
      "[Batch #79000/122727] Loss: 1367.7133786678314\n",
      "[Batch #79500/122727] Loss: 1359.901216506958\n",
      "[Batch #80000/122727] Loss: 1373.1297600269318\n",
      "[Batch #80500/122727] Loss: 1372.7512395381927\n",
      "[Batch #81000/122727] Loss: 1370.1702816486359\n",
      "[Batch #81500/122727] Loss: 1370.0754537582397\n",
      "[Batch #82000/122727] Loss: 1371.5855512619019\n",
      "[Batch #82500/122727] Loss: 1377.9358756542206\n",
      "[Batch #83000/122727] Loss: 1364.2478647232056\n",
      "[Batch #83500/122727] Loss: 1365.5850670337677\n",
      "[Batch #84000/122727] Loss: 1368.8973356485367\n",
      "[Batch #84500/122727] Loss: 1365.5789635181427\n",
      "[Batch #85000/122727] Loss: 1369.452730178833\n",
      "[Batch #85500/122727] Loss: 1376.0032696723938\n",
      "[Batch #86000/122727] Loss: 1368.1582252979279\n",
      "[Batch #86500/122727] Loss: 1370.1228659152985\n",
      "[Batch #87000/122727] Loss: 1373.544443845749\n",
      "[Batch #87500/122727] Loss: 1366.2360534667969\n",
      "[Batch #88000/122727] Loss: 1372.3928537368774\n",
      "[Batch #88500/122727] Loss: 1369.420286655426\n",
      "[Batch #89000/122727] Loss: 1365.1598801612854\n",
      "[Batch #89500/122727] Loss: 1366.5378692150116\n",
      "[Batch #90000/122727] Loss: 1361.8143191337585\n",
      "[Batch #90500/122727] Loss: 1360.3694972991943\n",
      "[Batch #91000/122727] Loss: 1374.9366083145142\n",
      "[Batch #91500/122727] Loss: 1372.9479250907898\n",
      "[Batch #92000/122727] Loss: 1365.5991792678833\n",
      "[Batch #92500/122727] Loss: 1371.2053253650665\n",
      "[Batch #93000/122727] Loss: 1359.6406726837158\n",
      "[Batch #93500/122727] Loss: 1367.3986072540283\n",
      "[Batch #94000/122727] Loss: 1369.017117023468\n",
      "[Batch #94500/122727] Loss: 1363.7103533744812\n",
      "[Batch #95000/122727] Loss: 1377.0594820976257\n",
      "[Batch #95500/122727] Loss: 1362.280419588089\n",
      "[Batch #96000/122727] Loss: 1377.8648948669434\n",
      "[Batch #96500/122727] Loss: 1372.4204411506653\n",
      "[Batch #97000/122727] Loss: 1370.0074954032898\n",
      "[Batch #97500/122727] Loss: 1366.4437801837921\n",
      "[Batch #98000/122727] Loss: 1363.2284018993378\n",
      "[Batch #98500/122727] Loss: 1372.395399093628\n",
      "[Batch #99000/122727] Loss: 1369.0365765094757\n",
      "[Batch #99500/122727] Loss: 1364.1949152946472\n",
      "[Batch #100000/122727] Loss: 1367.3864431381226\n",
      "[Batch #100500/122727] Loss: 1368.1040637493134\n",
      "[Batch #101000/122727] Loss: 1367.1984996795654\n",
      "[Batch #101500/122727] Loss: 1369.184100151062\n",
      "[Batch #102000/122727] Loss: 1365.057686805725\n",
      "[Batch #102500/122727] Loss: 1364.6887941360474\n",
      "[Batch #103000/122727] Loss: 1370.389539718628\n",
      "[Batch #103500/122727] Loss: 1363.5622856616974\n",
      "[Batch #104000/122727] Loss: 1368.6864144802094\n",
      "[Batch #104500/122727] Loss: 1364.7252464294434\n",
      "[Batch #105000/122727] Loss: 1361.8395709991455\n",
      "[Batch #105500/122727] Loss: 1371.1249933242798\n",
      "[Batch #106000/122727] Loss: 1369.828722000122\n",
      "[Batch #106500/122727] Loss: 1363.3422396183014\n",
      "[Batch #107000/122727] Loss: 1370.3469967842102\n",
      "[Batch #107500/122727] Loss: 1363.9395176172256\n",
      "[Batch #108000/122727] Loss: 1368.6978123188019\n",
      "[Batch #108500/122727] Loss: 1367.9058310985565\n",
      "[Batch #109000/122727] Loss: 1369.6313664913177\n",
      "[Batch #109500/122727] Loss: 1365.004693031311\n",
      "[Batch #110000/122727] Loss: 1372.951626777649\n",
      "[Batch #110500/122727] Loss: 1369.8382818698883\n",
      "[Batch #111000/122727] Loss: 1367.939913034439\n",
      "[Batch #111500/122727] Loss: 1366.5371828079224\n",
      "[Batch #112000/122727] Loss: 1366.6049337387085\n",
      "[Batch #112500/122727] Loss: 1365.7908029556274\n",
      "[Batch #113000/122727] Loss: 1359.2186084985733\n",
      "[Batch #113500/122727] Loss: 1362.6638495922089\n",
      "[Batch #114000/122727] Loss: 1359.7217445373535\n",
      "[Batch #114500/122727] Loss: 1361.5035322904587\n",
      "[Batch #115000/122727] Loss: 1369.7267122268677\n",
      "[Batch #115500/122727] Loss: 1359.7144720554352\n",
      "[Batch #116000/122727] Loss: 1373.2092452049255\n",
      "[Batch #116500/122727] Loss: 1365.1829345226288\n",
      "[Batch #117000/122727] Loss: 1374.0297920703888\n",
      "[Batch #117500/122727] Loss: 1370.3600590229034\n",
      "[Batch #118000/122727] Loss: 1365.9081256389618\n",
      "[Batch #118500/122727] Loss: 1356.5832154750824\n",
      "[Batch #119000/122727] Loss: 1368.1253929138184\n",
      "[Batch #119500/122727] Loss: 1377.0336802005768\n",
      "[Batch #120000/122727] Loss: 1363.1620062589645\n",
      "[Batch #120500/122727] Loss: 1369.4426591396332\n",
      "[Batch #121000/122727] Loss: 1370.3916392326355\n",
      "[Batch #121500/122727] Loss: 1362.2620749473572\n",
      "[Batch #122000/122727] Loss: 1375.9093363285065\n",
      "[Batch #122500/122727] Loss: 1354.2388060092926\n",
      "\n",
      "Validating model...\n",
      "Epoch 1 complete. Training loss: 2.7367083663755953 - Validation loss: 2.5489478751349965\n",
      "----- END OF EPOCH 1 -----\n",
      "\n",
      "New best model found. Saving...\n",
      "----- STARTING EPOCH 2 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1357.387023806572\n",
      "[Batch #1000/122727] Loss: 1371.1847064495087\n",
      "[Batch #1500/122727] Loss: 1370.533210992813\n",
      "[Batch #2000/122727] Loss: 1371.2627141475677\n",
      "[Batch #2500/122727] Loss: 1361.304273366928\n",
      "[Batch #3000/122727] Loss: 1373.248354434967\n",
      "[Batch #3500/122727] Loss: 1367.8363041877747\n",
      "[Batch #4000/122727] Loss: 1365.4202268123627\n",
      "[Batch #4500/122727] Loss: 1362.0031465291977\n",
      "[Batch #5000/122727] Loss: 1363.683359861374\n",
      "[Batch #5500/122727] Loss: 1362.5461621284485\n",
      "[Batch #6000/122727] Loss: 1371.279143691063\n",
      "[Batch #6500/122727] Loss: 1364.383244752884\n",
      "[Batch #7000/122727] Loss: 1370.6713137626648\n",
      "[Batch #7500/122727] Loss: 1365.97154545784\n",
      "[Batch #8000/122727] Loss: 1371.348480463028\n",
      "[Batch #8500/122727] Loss: 1357.5269169807434\n",
      "[Batch #9000/122727] Loss: 1369.26944231987\n",
      "[Batch #9500/122727] Loss: 1372.8772234916687\n",
      "[Batch #10000/122727] Loss: 1364.8002762794495\n",
      "[Batch #10500/122727] Loss: 1372.0699090957642\n",
      "[Batch #11000/122727] Loss: 1362.4200904369354\n",
      "[Batch #11500/122727] Loss: 1368.7656672000885\n",
      "[Batch #12000/122727] Loss: 1360.975816488266\n",
      "[Batch #12500/122727] Loss: 1366.3783712387085\n",
      "[Batch #13000/122727] Loss: 1369.932024717331\n",
      "[Batch #13500/122727] Loss: 1369.0219218730927\n",
      "[Batch #14000/122727] Loss: 1361.949847459793\n",
      "[Batch #14500/122727] Loss: 1365.4200580120087\n",
      "[Batch #15000/122727] Loss: 1373.2129666805267\n",
      "[Batch #15500/122727] Loss: 1372.1301724910736\n",
      "[Batch #16000/122727] Loss: 1368.8101918697357\n",
      "[Batch #16500/122727] Loss: 1363.6657605171204\n",
      "[Batch #17000/122727] Loss: 1365.125347852707\n",
      "[Batch #17500/122727] Loss: 1363.5940395593643\n",
      "[Batch #18000/122727] Loss: 1375.546877861023\n",
      "[Batch #18500/122727] Loss: 1356.6292815208435\n",
      "[Batch #19000/122727] Loss: 1370.9635472297668\n",
      "[Batch #19500/122727] Loss: 1376.0574769973755\n",
      "[Batch #20000/122727] Loss: 1373.8356405496597\n",
      "[Batch #20500/122727] Loss: 1359.9089131355286\n",
      "[Batch #21000/122727] Loss: 1362.531733751297\n",
      "[Batch #21500/122727] Loss: 1362.6289014816284\n",
      "[Batch #22000/122727] Loss: 1369.998003602028\n",
      "[Batch #22500/122727] Loss: 1365.5028595924377\n",
      "[Batch #23000/122727] Loss: 1360.2956817150116\n",
      "[Batch #23500/122727] Loss: 1362.3037021160126\n",
      "[Batch #24000/122727] Loss: 1364.1774280071259\n",
      "[Batch #24500/122727] Loss: 1367.4282976388931\n",
      "[Batch #25000/122727] Loss: 1365.9482152462006\n",
      "[Batch #25500/122727] Loss: 1361.726721048355\n",
      "[Batch #26000/122727] Loss: 1361.8914535045624\n",
      "[Batch #26500/122727] Loss: 1371.057674407959\n",
      "[Batch #27000/122727] Loss: 1362.792132616043\n",
      "[Batch #27500/122727] Loss: 1366.5594408512115\n",
      "[Batch #28000/122727] Loss: 1370.5971932411194\n",
      "[Batch #28500/122727] Loss: 1366.950412273407\n",
      "[Batch #29000/122727] Loss: 1355.330750465393\n",
      "[Batch #29500/122727] Loss: 1365.6629207134247\n",
      "[Batch #30000/122727] Loss: 1371.1361638307571\n",
      "[Batch #30500/122727] Loss: 1361.6869525909424\n",
      "[Batch #31000/122727] Loss: 1367.8407011032104\n",
      "[Batch #31500/122727] Loss: 1362.968393921852\n",
      "[Batch #32000/122727] Loss: 1371.1837327480316\n",
      "[Batch #32500/122727] Loss: 1361.0811338424683\n",
      "[Batch #33000/122727] Loss: 1374.8058443069458\n",
      "[Batch #33500/122727] Loss: 1375.4458520412445\n",
      "[Batch #34000/122727] Loss: 1371.039037823677\n",
      "[Batch #34500/122727] Loss: 1359.4475281238556\n",
      "[Batch #35000/122727] Loss: 1360.7131123542786\n",
      "[Batch #35500/122727] Loss: 1361.7068355083466\n",
      "[Batch #36000/122727] Loss: 1358.3616744279861\n",
      "[Batch #36500/122727] Loss: 1361.9162471294403\n",
      "[Batch #37000/122727] Loss: 1370.7115316390991\n",
      "[Batch #37500/122727] Loss: 1364.2097518444061\n",
      "[Batch #38000/122727] Loss: 1360.2381706237793\n",
      "[Batch #38500/122727] Loss: 1357.2633745670319\n",
      "[Batch #39000/122727] Loss: 1368.0402073860168\n",
      "[Batch #39500/122727] Loss: 1370.6023486852646\n",
      "[Batch #40000/122727] Loss: 1358.1707544326782\n",
      "[Batch #40500/122727] Loss: 1365.6102621555328\n",
      "[Batch #41000/122727] Loss: 1359.3809638023376\n",
      "[Batch #41500/122727] Loss: 1360.631704568863\n",
      "[Batch #42000/122727] Loss: 1370.6987228393555\n",
      "[Batch #42500/122727] Loss: 1368.0248517990112\n",
      "[Batch #43000/122727] Loss: 1371.3579494953156\n",
      "[Batch #43500/122727] Loss: 1362.9030141830444\n",
      "[Batch #44000/122727] Loss: 1370.4660015106201\n",
      "[Batch #44500/122727] Loss: 1366.6519525051117\n",
      "[Batch #45000/122727] Loss: 1371.5843136310577\n",
      "[Batch #45500/122727] Loss: 1371.4035034179688\n",
      "[Batch #46000/122727] Loss: 1365.2275161743164\n",
      "[Batch #46500/122727] Loss: 1367.727467060089\n",
      "[Batch #47000/122727] Loss: 1367.4875383377075\n",
      "[Batch #47500/122727] Loss: 1367.620519399643\n",
      "[Batch #48000/122727] Loss: 1367.6057748794556\n",
      "[Batch #48500/122727] Loss: 1368.410397052765\n",
      "[Batch #49000/122727] Loss: 1372.9171398878098\n",
      "[Batch #49500/122727] Loss: 1363.6358199119568\n",
      "[Batch #50000/122727] Loss: 1365.643876671791\n",
      "[Batch #50500/122727] Loss: 1369.8020951747894\n",
      "[Batch #51000/122727] Loss: 1372.7929167747498\n",
      "[Batch #51500/122727] Loss: 1368.2902591228485\n",
      "[Batch #52000/122727] Loss: 1364.07395195961\n",
      "[Batch #52500/122727] Loss: 1367.2483761310577\n",
      "[Batch #53000/122727] Loss: 1369.0516834259033\n",
      "[Batch #53500/122727] Loss: 1375.2189366817474\n",
      "[Batch #54000/122727] Loss: 1358.7693676948547\n",
      "[Batch #54500/122727] Loss: 1356.0688211917877\n",
      "[Batch #55000/122727] Loss: 1366.7799401283264\n",
      "[Batch #55500/122727] Loss: 1363.803572177887\n",
      "[Batch #56000/122727] Loss: 1361.1252675056458\n",
      "[Batch #56500/122727] Loss: 1368.1191358566284\n",
      "[Batch #57000/122727] Loss: 1361.419444322586\n",
      "[Batch #57500/122727] Loss: 1364.865161895752\n",
      "[Batch #58000/122727] Loss: 1362.339281320572\n",
      "[Batch #58500/122727] Loss: 1370.4303522109985\n",
      "[Batch #59000/122727] Loss: 1379.2106750011444\n",
      "[Batch #59500/122727] Loss: 1366.0390280485153\n",
      "[Batch #60000/122727] Loss: 1360.9961881637573\n",
      "[Batch #60500/122727] Loss: 1372.6807789802551\n",
      "[Batch #61000/122727] Loss: 1361.0358493328094\n",
      "[Batch #61500/122727] Loss: 1356.4171152114868\n",
      "[Batch #62000/122727] Loss: 1364.562143445015\n",
      "[Batch #62500/122727] Loss: 1369.457947731018\n",
      "[Batch #63000/122727] Loss: 1364.0965626239777\n",
      "[Batch #63500/122727] Loss: 1370.3421790599823\n",
      "[Batch #64000/122727] Loss: 1363.4723386764526\n",
      "[Batch #64500/122727] Loss: 1366.424716949463\n",
      "[Batch #65000/122727] Loss: 1363.959327697754\n",
      "[Batch #65500/122727] Loss: 1370.6211183071136\n",
      "[Batch #66000/122727] Loss: 1359.829978942871\n",
      "[Batch #66500/122727] Loss: 1363.3908050060272\n",
      "[Batch #67000/122727] Loss: 1357.4979603290558\n",
      "[Batch #67500/122727] Loss: 1364.9044873714447\n",
      "[Batch #68000/122727] Loss: 1361.588376045227\n",
      "[Batch #68500/122727] Loss: 1371.1396894454956\n",
      "[Batch #69000/122727] Loss: 1368.5360872745514\n",
      "[Batch #69500/122727] Loss: 1363.014063835144\n",
      "[Batch #70000/122727] Loss: 1363.6779298782349\n",
      "[Batch #70500/122727] Loss: 1369.0991787910461\n",
      "[Batch #71000/122727] Loss: 1374.1625082492828\n",
      "[Batch #71500/122727] Loss: 1362.3548061847687\n",
      "[Batch #72000/122727] Loss: 1368.3022463321686\n",
      "[Batch #72500/122727] Loss: 1359.3652002811432\n",
      "[Batch #73000/122727] Loss: 1360.3200919628143\n",
      "[Batch #73500/122727] Loss: 1375.1043951511383\n",
      "[Batch #74000/122727] Loss: 1371.088360786438\n",
      "[Batch #74500/122727] Loss: 1377.9138793945312\n",
      "[Batch #75000/122727] Loss: 1362.3696358203888\n",
      "[Batch #75500/122727] Loss: 1367.3931663036346\n",
      "[Batch #76000/122727] Loss: 1367.2777614593506\n",
      "[Batch #76500/122727] Loss: 1360.0655566453934\n",
      "[Batch #77000/122727] Loss: 1362.2253649234772\n",
      "[Batch #77500/122727] Loss: 1368.0225521326065\n",
      "[Batch #78000/122727] Loss: 1356.2887721061707\n",
      "[Batch #78500/122727] Loss: 1363.1374232769012\n",
      "[Batch #79000/122727] Loss: 1361.0198023319244\n",
      "[Batch #79500/122727] Loss: 1355.897761106491\n",
      "[Batch #80000/122727] Loss: 1365.608039855957\n",
      "[Batch #80500/122727] Loss: 1351.347639799118\n",
      "[Batch #81000/122727] Loss: 1365.5724875926971\n",
      "[Batch #81500/122727] Loss: 1355.615689754486\n",
      "[Batch #82000/122727] Loss: 1364.9640185832977\n",
      "[Batch #82500/122727] Loss: 1366.6067135334015\n",
      "[Batch #83000/122727] Loss: 1372.6500062942505\n",
      "[Batch #83500/122727] Loss: 1361.8931846618652\n",
      "[Batch #84000/122727] Loss: 1357.7405831813812\n",
      "[Batch #84500/122727] Loss: 1362.635486125946\n",
      "[Batch #85000/122727] Loss: 1354.3689754009247\n",
      "[Batch #85500/122727] Loss: 1366.1896579265594\n",
      "[Batch #86000/122727] Loss: 1371.7124516963959\n",
      "[Batch #86500/122727] Loss: 1359.5215787887573\n",
      "[Batch #87000/122727] Loss: 1369.0785701274872\n",
      "[Batch #87500/122727] Loss: 1372.2569043636322\n",
      "[Batch #88000/122727] Loss: 1369.1529290676117\n",
      "[Batch #88500/122727] Loss: 1362.6633286476135\n",
      "[Batch #89000/122727] Loss: 1370.9500150680542\n",
      "[Batch #89500/122727] Loss: 1366.8081612586975\n",
      "[Batch #90000/122727] Loss: 1366.6033017635345\n",
      "[Batch #90500/122727] Loss: 1359.4052681922913\n",
      "[Batch #91000/122727] Loss: 1369.7599349021912\n",
      "[Batch #91500/122727] Loss: 1366.7549961805344\n",
      "[Batch #92000/122727] Loss: 1374.4155459403992\n",
      "[Batch #92500/122727] Loss: 1360.8695915937424\n",
      "[Batch #93000/122727] Loss: 1355.1043634414673\n",
      "[Batch #93500/122727] Loss: 1359.4724543094635\n",
      "[Batch #94000/122727] Loss: 1366.3897078037262\n",
      "[Batch #94500/122727] Loss: 1375.0658555030823\n",
      "[Batch #95000/122727] Loss: 1366.8788616657257\n",
      "[Batch #95500/122727] Loss: 1370.618281841278\n",
      "[Batch #96000/122727] Loss: 1362.2487151622772\n",
      "[Batch #96500/122727] Loss: 1367.9964101314545\n",
      "[Batch #97000/122727] Loss: 1358.2232409715652\n",
      "[Batch #97500/122727] Loss: 1366.9696011543274\n",
      "[Batch #98000/122727] Loss: 1366.7774698734283\n",
      "[Batch #98500/122727] Loss: 1365.5756130218506\n",
      "[Batch #99000/122727] Loss: 1369.549134016037\n",
      "[Batch #99500/122727] Loss: 1369.1946685314178\n",
      "[Batch #100000/122727] Loss: 1359.266774892807\n",
      "[Batch #100500/122727] Loss: 1374.8244090080261\n",
      "[Batch #101000/122727] Loss: 1363.7186765670776\n",
      "[Batch #101500/122727] Loss: 1367.0733988285065\n",
      "[Batch #102000/122727] Loss: 1359.3818907737732\n",
      "[Batch #102500/122727] Loss: 1369.0176490545273\n",
      "[Batch #103000/122727] Loss: 1353.2842817306519\n",
      "[Batch #103500/122727] Loss: 1366.8243606090546\n",
      "[Batch #104000/122727] Loss: 1354.8976011276245\n",
      "[Batch #104500/122727] Loss: 1367.9911530017853\n",
      "[Batch #105000/122727] Loss: 1365.941844701767\n",
      "[Batch #105500/122727] Loss: 1365.7002258300781\n",
      "[Batch #106000/122727] Loss: 1361.2671053409576\n",
      "[Batch #106500/122727] Loss: 1363.0348989963531\n",
      "[Batch #107000/122727] Loss: 1375.7925777435303\n",
      "[Batch #107500/122727] Loss: 1359.526920080185\n",
      "[Batch #108000/122727] Loss: 1362.8029494285583\n",
      "[Batch #108500/122727] Loss: 1363.15500831604\n",
      "[Batch #109000/122727] Loss: 1367.3681268692017\n",
      "[Batch #109500/122727] Loss: 1361.3134717941284\n",
      "[Batch #110000/122727] Loss: 1369.8370220661163\n",
      "[Batch #110500/122727] Loss: 1372.5138972997665\n",
      "[Batch #111000/122727] Loss: 1371.722142457962\n",
      "[Batch #111500/122727] Loss: 1362.374951839447\n",
      "[Batch #112000/122727] Loss: 1357.9407234191895\n",
      "[Batch #112500/122727] Loss: 1365.0194602012634\n",
      "[Batch #113000/122727] Loss: 1366.925006866455\n",
      "[Batch #113500/122727] Loss: 1365.34862947464\n",
      "[Batch #114000/122727] Loss: 1354.6060395240784\n",
      "[Batch #114500/122727] Loss: 1364.832393169403\n",
      "[Batch #115000/122727] Loss: 1359.0594930648804\n",
      "[Batch #115500/122727] Loss: 1361.9221169948578\n",
      "[Batch #116000/122727] Loss: 1362.9100198745728\n",
      "[Batch #116500/122727] Loss: 1377.0700466632843\n",
      "[Batch #117000/122727] Loss: 1371.1467390060425\n",
      "[Batch #117500/122727] Loss: 1369.8957619667053\n",
      "[Batch #118000/122727] Loss: 1361.8970122337341\n",
      "[Batch #118500/122727] Loss: 1366.4211032390594\n",
      "[Batch #119000/122727] Loss: 1359.4581925868988\n",
      "[Batch #119500/122727] Loss: 1357.0501186847687\n",
      "[Batch #120000/122727] Loss: 1367.3459522724152\n",
      "[Batch #120500/122727] Loss: 1363.7493042945862\n",
      "[Batch #121000/122727] Loss: 1374.8968749046326\n",
      "[Batch #121500/122727] Loss: 1367.1374628543854\n",
      "[Batch #122000/122727] Loss: 1355.8791897296906\n",
      "[Batch #122500/122727] Loss: 1365.2240664958954\n",
      "\n",
      "Validating model...\n",
      "Epoch 2 complete. Training loss: 2.731280152397916 - Validation loss: 2.557781790668895\n",
      "----- END OF EPOCH 2 -----\n",
      "\n",
      "----- STARTING EPOCH 3 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1356.0129373073578\n",
      "[Batch #1000/122727] Loss: 1371.4062082767487\n",
      "[Batch #1500/122727] Loss: 1366.4801032543182\n",
      "[Batch #2000/122727] Loss: 1360.8720145225525\n",
      "[Batch #2500/122727] Loss: 1360.9041075706482\n",
      "[Batch #3000/122727] Loss: 1370.0107097625732\n",
      "[Batch #3500/122727] Loss: 1371.8932371139526\n",
      "[Batch #4000/122727] Loss: 1367.7482788562775\n",
      "[Batch #4500/122727] Loss: 1360.993569135666\n",
      "[Batch #5000/122727] Loss: 1358.9432904720306\n",
      "[Batch #5500/122727] Loss: 1365.5362334251404\n",
      "[Batch #6000/122727] Loss: 1373.022834777832\n",
      "[Batch #6500/122727] Loss: 1370.4515589475632\n",
      "[Batch #7000/122727] Loss: 1362.5312497615814\n",
      "[Batch #7500/122727] Loss: 1362.8383524417877\n",
      "[Batch #8000/122727] Loss: 1364.7835018634796\n",
      "[Batch #8500/122727] Loss: 1366.289860010147\n",
      "[Batch #9000/122727] Loss: 1361.020785331726\n",
      "[Batch #9500/122727] Loss: 1369.6895351409912\n",
      "[Batch #10000/122727] Loss: 1366.4311091899872\n",
      "[Batch #10500/122727] Loss: 1364.1897277832031\n",
      "[Batch #11000/122727] Loss: 1372.4799375534058\n",
      "[Batch #11500/122727] Loss: 1364.690083026886\n",
      "[Batch #12000/122727] Loss: 1372.098444223404\n",
      "[Batch #12500/122727] Loss: 1360.9572129249573\n",
      "[Batch #13000/122727] Loss: 1355.8629729747772\n",
      "[Batch #13500/122727] Loss: 1358.2024366855621\n",
      "[Batch #14000/122727] Loss: 1365.7817273139954\n",
      "[Batch #14500/122727] Loss: 1353.8603718280792\n",
      "[Batch #15000/122727] Loss: 1362.3605065345764\n",
      "[Batch #15500/122727] Loss: 1368.8273779153824\n",
      "[Batch #16000/122727] Loss: 1367.4467446804047\n",
      "[Batch #16500/122727] Loss: 1365.8485162258148\n",
      "[Batch #17000/122727] Loss: 1365.6955397129059\n",
      "[Batch #17500/122727] Loss: 1367.454005599022\n",
      "[Batch #18000/122727] Loss: 1361.8724012374878\n",
      "[Batch #18500/122727] Loss: 1364.2405788898468\n",
      "[Batch #19000/122727] Loss: 1363.5978963375092\n",
      "[Batch #19500/122727] Loss: 1368.3006093502045\n",
      "[Batch #20000/122727] Loss: 1359.2074959278107\n",
      "[Batch #20500/122727] Loss: 1363.0397737026215\n",
      "[Batch #21000/122727] Loss: 1361.5371887683868\n",
      "[Batch #21500/122727] Loss: 1360.4556498527527\n",
      "[Batch #22000/122727] Loss: 1355.239019870758\n",
      "[Batch #22500/122727] Loss: 1365.5698926448822\n",
      "[Batch #23000/122727] Loss: 1372.5988173484802\n",
      "[Batch #23500/122727] Loss: 1364.9434051513672\n",
      "[Batch #24000/122727] Loss: 1364.6549410820007\n",
      "[Batch #24500/122727] Loss: 1367.655431985855\n",
      "[Batch #25000/122727] Loss: 1358.7899885177612\n",
      "[Batch #25500/122727] Loss: 1367.2456846237183\n",
      "[Batch #26000/122727] Loss: 1367.45987200737\n",
      "[Batch #26500/122727] Loss: 1359.9462270736694\n",
      "[Batch #27000/122727] Loss: 1364.3766522407532\n",
      "[Batch #27500/122727] Loss: 1370.032565355301\n",
      "[Batch #28000/122727] Loss: 1361.0556026697159\n",
      "[Batch #28500/122727] Loss: 1358.9497249126434\n",
      "[Batch #29000/122727] Loss: 1372.3456852436066\n",
      "[Batch #29500/122727] Loss: 1363.4582595825195\n",
      "[Batch #30000/122727] Loss: 1360.6241557598114\n",
      "[Batch #30500/122727] Loss: 1363.1167929172516\n",
      "[Batch #31000/122727] Loss: 1361.2977316379547\n",
      "[Batch #31500/122727] Loss: 1370.2320938110352\n",
      "[Batch #32000/122727] Loss: 1362.4796352386475\n",
      "[Batch #32500/122727] Loss: 1359.2527894973755\n",
      "[Batch #33000/122727] Loss: 1364.4418247938156\n",
      "[Batch #33500/122727] Loss: 1364.9430010318756\n",
      "[Batch #34000/122727] Loss: 1366.0040237903595\n",
      "[Batch #34500/122727] Loss: 1360.2081928253174\n",
      "[Batch #35000/122727] Loss: 1364.5122129917145\n",
      "[Batch #35500/122727] Loss: 1365.6008274555206\n",
      "[Batch #36000/122727] Loss: 1363.575166940689\n",
      "[Batch #36500/122727] Loss: 1356.8022615909576\n",
      "[Batch #37000/122727] Loss: 1362.2852034568787\n",
      "[Batch #37500/122727] Loss: 1356.4405233860016\n",
      "[Batch #38000/122727] Loss: 1372.4798176288605\n",
      "[Batch #38500/122727] Loss: 1367.9206223487854\n",
      "[Batch #39000/122727] Loss: 1359.2334806919098\n",
      "[Batch #39500/122727] Loss: 1362.425056219101\n",
      "[Batch #40000/122727] Loss: 1361.9424946308136\n",
      "[Batch #40500/122727] Loss: 1364.7633385658264\n",
      "[Batch #41000/122727] Loss: 1365.8577646017075\n",
      "[Batch #41500/122727] Loss: 1371.1646237373352\n",
      "[Batch #42000/122727] Loss: 1362.660181760788\n",
      "[Batch #42500/122727] Loss: 1372.564265012741\n",
      "[Batch #43000/122727] Loss: 1360.3259961605072\n",
      "[Batch #43500/122727] Loss: 1357.065685391426\n",
      "[Batch #44000/122727] Loss: 1370.635828256607\n",
      "[Batch #44500/122727] Loss: 1366.888554096222\n",
      "[Batch #45000/122727] Loss: 1362.908254146576\n",
      "[Batch #45500/122727] Loss: 1357.5238842964172\n",
      "[Batch #46000/122727] Loss: 1364.5358783006668\n",
      "[Batch #46500/122727] Loss: 1362.9532794952393\n",
      "[Batch #47000/122727] Loss: 1363.550731420517\n",
      "[Batch #47500/122727] Loss: 1367.422914147377\n",
      "[Batch #48000/122727] Loss: 1369.5782642364502\n",
      "[Batch #48500/122727] Loss: 1365.1826176643372\n",
      "[Batch #49000/122727] Loss: 1362.2764196395874\n",
      "[Batch #49500/122727] Loss: 1358.0713061094284\n",
      "[Batch #50000/122727] Loss: 1363.475170135498\n",
      "[Batch #50500/122727] Loss: 1363.9150731563568\n",
      "[Batch #51000/122727] Loss: 1366.3548259735107\n",
      "[Batch #51500/122727] Loss: 1366.4095132350922\n",
      "[Batch #52000/122727] Loss: 1362.8012189865112\n",
      "[Batch #52500/122727] Loss: 1366.300297498703\n",
      "[Batch #53000/122727] Loss: 1370.48344540596\n",
      "[Batch #53500/122727] Loss: 1358.2479884624481\n",
      "[Batch #54000/122727] Loss: 1356.8512964248657\n",
      "[Batch #54500/122727] Loss: 1359.4427559375763\n",
      "[Batch #55000/122727] Loss: 1363.82803440094\n",
      "[Batch #55500/122727] Loss: 1362.3555307388306\n",
      "[Batch #56000/122727] Loss: 1367.2249970436096\n",
      "[Batch #56500/122727] Loss: 1367.9414932727814\n",
      "[Batch #57000/122727] Loss: 1363.9154291152954\n",
      "[Batch #57500/122727] Loss: 1369.3132152557373\n",
      "[Batch #58000/122727] Loss: 1351.1581270694733\n",
      "[Batch #58500/122727] Loss: 1368.9074273109436\n",
      "[Batch #59000/122727] Loss: 1361.6889317035675\n",
      "[Batch #59500/122727] Loss: 1362.6428635120392\n",
      "[Batch #60000/122727] Loss: 1365.8976998329163\n",
      "[Batch #60500/122727] Loss: 1359.25505900383\n",
      "[Batch #61000/122727] Loss: 1362.5006728172302\n",
      "[Batch #61500/122727] Loss: 1365.7586245536804\n",
      "[Batch #62000/122727] Loss: 1373.6597111225128\n",
      "[Batch #62500/122727] Loss: 1358.6428775787354\n",
      "[Batch #63000/122727] Loss: 1365.212994813919\n",
      "[Batch #63500/122727] Loss: 1366.3509953022003\n",
      "[Batch #64000/122727] Loss: 1367.4186379909515\n",
      "[Batch #64500/122727] Loss: 1357.6790124177933\n",
      "[Batch #65000/122727] Loss: 1359.8707485198975\n",
      "[Batch #65500/122727] Loss: 1366.7007277011871\n",
      "[Batch #66000/122727] Loss: 1353.0139575004578\n",
      "[Batch #66500/122727] Loss: 1362.2725269794464\n",
      "[Batch #67000/122727] Loss: 1358.1474891901016\n",
      "[Batch #67500/122727] Loss: 1366.9902032613754\n",
      "[Batch #68000/122727] Loss: 1376.864986896515\n",
      "[Batch #68500/122727] Loss: 1362.8414249420166\n",
      "[Batch #69000/122727] Loss: 1363.9261820316315\n",
      "[Batch #69500/122727] Loss: 1362.8296043872833\n",
      "[Batch #70000/122727] Loss: 1361.6530196666718\n",
      "[Batch #70500/122727] Loss: 1359.1505942344666\n",
      "[Batch #71000/122727] Loss: 1366.8442316055298\n",
      "[Batch #71500/122727] Loss: 1361.2292563915253\n",
      "[Batch #72000/122727] Loss: 1372.99276638031\n",
      "[Batch #72500/122727] Loss: 1362.294888496399\n",
      "[Batch #73000/122727] Loss: 1371.2911970615387\n",
      "[Batch #73500/122727] Loss: 1366.623083114624\n",
      "[Batch #74000/122727] Loss: 1365.1003603935242\n",
      "[Batch #74500/122727] Loss: 1362.2395009994507\n",
      "[Batch #75000/122727] Loss: 1370.6090774536133\n",
      "[Batch #75500/122727] Loss: 1366.586812376976\n",
      "[Batch #76000/122727] Loss: 1363.8262808322906\n",
      "[Batch #76500/122727] Loss: 1358.9225243330002\n",
      "[Batch #77000/122727] Loss: 1365.751636505127\n",
      "[Batch #77500/122727] Loss: 1364.056801557541\n",
      "[Batch #78000/122727] Loss: 1370.7415301799774\n",
      "[Batch #78500/122727] Loss: 1369.1647462844849\n",
      "[Batch #79000/122727] Loss: 1359.2555282115936\n",
      "[Batch #79500/122727] Loss: 1365.8719170093536\n",
      "[Batch #80000/122727] Loss: 1378.6542205810547\n",
      "[Batch #80500/122727] Loss: 1360.5929095745087\n",
      "[Batch #81000/122727] Loss: 1365.6481521129608\n",
      "[Batch #81500/122727] Loss: 1360.3718601465225\n",
      "[Batch #82000/122727] Loss: 1371.0634589195251\n",
      "[Batch #82500/122727] Loss: 1359.9021599292755\n",
      "[Batch #83000/122727] Loss: 1366.2714173793793\n",
      "[Batch #83500/122727] Loss: 1365.3401749134064\n",
      "[Batch #84000/122727] Loss: 1362.7530580759048\n",
      "[Batch #84500/122727] Loss: 1369.4911105632782\n",
      "[Batch #85000/122727] Loss: 1363.1375260353088\n",
      "[Batch #85500/122727] Loss: 1370.0750918388367\n",
      "[Batch #86000/122727] Loss: 1365.525486946106\n",
      "[Batch #86500/122727] Loss: 1370.874679327011\n",
      "[Batch #87000/122727] Loss: 1368.0756412744522\n",
      "[Batch #87500/122727] Loss: 1361.6936593055725\n",
      "[Batch #88000/122727] Loss: 1365.0247757434845\n",
      "[Batch #88500/122727] Loss: 1369.6081144809723\n",
      "[Batch #89000/122727] Loss: 1362.2428698539734\n",
      "[Batch #89500/122727] Loss: 1361.966811656952\n",
      "[Batch #90000/122727] Loss: 1357.9626564979553\n",
      "[Batch #90500/122727] Loss: 1363.0240271091461\n",
      "[Batch #91000/122727] Loss: 1363.6101777553558\n",
      "[Batch #91500/122727] Loss: 1359.7235033512115\n",
      "[Batch #92000/122727] Loss: 1369.9577357769012\n",
      "[Batch #92500/122727] Loss: 1370.3109085559845\n",
      "[Batch #93000/122727] Loss: 1363.6996290683746\n",
      "[Batch #93500/122727] Loss: 1359.3320934772491\n",
      "[Batch #94000/122727] Loss: 1362.18667781353\n",
      "[Batch #94500/122727] Loss: 1363.1949533224106\n",
      "[Batch #95000/122727] Loss: 1368.421313047409\n",
      "[Batch #95500/122727] Loss: 1363.203349351883\n",
      "[Batch #96000/122727] Loss: 1351.6074132919312\n",
      "[Batch #96500/122727] Loss: 1350.9972693920135\n",
      "[Batch #97000/122727] Loss: 1351.4791407585144\n",
      "[Batch #97500/122727] Loss: 1365.363778591156\n",
      "[Batch #98000/122727] Loss: 1363.023677110672\n",
      "[Batch #98500/122727] Loss: 1362.392686367035\n",
      "[Batch #99000/122727] Loss: 1368.573221206665\n",
      "[Batch #99500/122727] Loss: 1357.2412333488464\n",
      "[Batch #100000/122727] Loss: 1360.487099289894\n",
      "[Batch #100500/122727] Loss: 1357.7194256782532\n",
      "[Batch #101000/122727] Loss: 1368.3156769275665\n",
      "[Batch #101500/122727] Loss: 1367.700230717659\n",
      "[Batch #102000/122727] Loss: 1357.8979749679565\n",
      "[Batch #102500/122727] Loss: 1356.9230893850327\n",
      "[Batch #103000/122727] Loss: 1363.697844028473\n",
      "[Batch #103500/122727] Loss: 1362.0660893917084\n",
      "[Batch #104000/122727] Loss: 1362.9124194383621\n",
      "[Batch #104500/122727] Loss: 1361.7125911712646\n",
      "[Batch #105000/122727] Loss: 1367.5355262756348\n",
      "[Batch #105500/122727] Loss: 1362.255089521408\n",
      "[Batch #106000/122727] Loss: 1364.6432702541351\n",
      "[Batch #106500/122727] Loss: 1363.62916970253\n",
      "[Batch #107000/122727] Loss: 1362.692759513855\n",
      "[Batch #107500/122727] Loss: 1369.9638494253159\n",
      "[Batch #108000/122727] Loss: 1369.8128101825714\n",
      "[Batch #108500/122727] Loss: 1371.0900617837906\n",
      "[Batch #109000/122727] Loss: 1363.6141910552979\n",
      "[Batch #109500/122727] Loss: 1361.2778532505035\n",
      "[Batch #110000/122727] Loss: 1369.2930698394775\n",
      "[Batch #110500/122727] Loss: 1359.587099313736\n",
      "[Batch #111000/122727] Loss: 1367.8848714828491\n",
      "[Batch #111500/122727] Loss: 1360.391155719757\n",
      "[Batch #112000/122727] Loss: 1367.3827397823334\n",
      "[Batch #112500/122727] Loss: 1368.0628635883331\n",
      "[Batch #113000/122727] Loss: 1367.271901845932\n",
      "[Batch #113500/122727] Loss: 1359.4461269378662\n",
      "[Batch #114000/122727] Loss: 1371.1159493923187\n",
      "[Batch #114500/122727] Loss: 1368.464126110077\n",
      "[Batch #115000/122727] Loss: 1362.9101996421814\n",
      "[Batch #115500/122727] Loss: 1371.402184009552\n",
      "[Batch #116000/122727] Loss: 1356.011269211769\n",
      "[Batch #116500/122727] Loss: 1362.393961906433\n",
      "[Batch #117000/122727] Loss: 1367.3292298316956\n",
      "[Batch #117500/122727] Loss: 1363.3116543293\n",
      "[Batch #118000/122727] Loss: 1367.0068373680115\n",
      "[Batch #118500/122727] Loss: 1370.5069509744644\n",
      "[Batch #119000/122727] Loss: 1365.3560628890991\n",
      "[Batch #119500/122727] Loss: 1361.94771194458\n",
      "[Batch #120000/122727] Loss: 1343.805496454239\n",
      "[Batch #120500/122727] Loss: 1364.3111605644226\n",
      "[Batch #121000/122727] Loss: 1371.1680936813354\n",
      "[Batch #121500/122727] Loss: 1368.7773089408875\n",
      "[Batch #122000/122727] Loss: 1359.1839046478271\n",
      "[Batch #122500/122727] Loss: 1369.898494720459\n",
      "\n",
      "Validating model...\n",
      "Epoch 3 complete. Training loss: 2.728424490801187 - Validation loss: 2.4928185178515374\n",
      "----- END OF EPOCH 3 -----\n",
      "\n",
      "New best model found. Saving...\n",
      "----- STARTING EPOCH 4 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1367.6231379508972\n",
      "[Batch #1000/122727] Loss: 1368.5941622257233\n",
      "[Batch #1500/122727] Loss: 1367.6928347349167\n",
      "[Batch #2000/122727] Loss: 1364.6837997436523\n",
      "[Batch #2500/122727] Loss: 1363.4734947681427\n",
      "[Batch #3000/122727] Loss: 1366.2505781650543\n",
      "[Batch #3500/122727] Loss: 1360.9732413291931\n",
      "[Batch #4000/122727] Loss: 1361.6758182048798\n",
      "[Batch #4500/122727] Loss: 1362.9798724651337\n",
      "[Batch #5000/122727] Loss: 1364.9858360290527\n",
      "[Batch #5500/122727] Loss: 1360.6099127531052\n",
      "[Batch #6000/122727] Loss: 1367.2780253887177\n",
      "[Batch #6500/122727] Loss: 1364.2715921401978\n",
      "[Batch #7000/122727] Loss: 1362.3430807590485\n",
      "[Batch #7500/122727] Loss: 1353.482440829277\n",
      "[Batch #8000/122727] Loss: 1365.1558322906494\n",
      "[Batch #8500/122727] Loss: 1362.3846130371094\n",
      "[Batch #9000/122727] Loss: 1358.776609659195\n",
      "[Batch #9500/122727] Loss: 1355.376205444336\n",
      "[Batch #10000/122727] Loss: 1366.0818364620209\n",
      "[Batch #10500/122727] Loss: 1362.8949930667877\n",
      "[Batch #11000/122727] Loss: 1360.2373485565186\n",
      "[Batch #11500/122727] Loss: 1352.94828748703\n",
      "[Batch #12000/122727] Loss: 1362.6783006191254\n",
      "[Batch #12500/122727] Loss: 1372.4175202846527\n",
      "[Batch #13000/122727] Loss: 1358.39173579216\n",
      "[Batch #13500/122727] Loss: 1360.7031247615814\n",
      "[Batch #14000/122727] Loss: 1351.4661328792572\n",
      "[Batch #14500/122727] Loss: 1372.623720407486\n",
      "[Batch #15000/122727] Loss: 1360.1989517211914\n",
      "[Batch #15500/122727] Loss: 1366.1060857772827\n",
      "[Batch #16000/122727] Loss: 1364.2743411064148\n",
      "[Batch #16500/122727] Loss: 1362.7558755874634\n",
      "[Batch #17000/122727] Loss: 1363.7098181247711\n",
      "[Batch #17500/122727] Loss: 1364.2252037525177\n",
      "[Batch #18000/122727] Loss: 1366.3897633552551\n",
      "[Batch #18500/122727] Loss: 1359.564549446106\n",
      "[Batch #19000/122727] Loss: 1353.6638379096985\n",
      "[Batch #19500/122727] Loss: 1368.4480835199356\n",
      "[Batch #20000/122727] Loss: 1358.8832604885101\n",
      "[Batch #20500/122727] Loss: 1365.9232716560364\n",
      "[Batch #21000/122727] Loss: 1360.5542793273926\n",
      "[Batch #21500/122727] Loss: 1370.4813137054443\n",
      "[Batch #22000/122727] Loss: 1363.8265824317932\n",
      "[Batch #22500/122727] Loss: 1355.9475790262222\n",
      "[Batch #23000/122727] Loss: 1363.0189876556396\n",
      "[Batch #23500/122727] Loss: 1361.4561403989792\n",
      "[Batch #24000/122727] Loss: 1349.4034037590027\n",
      "[Batch #24500/122727] Loss: 1364.2707166671753\n",
      "[Batch #25000/122727] Loss: 1361.1008484363556\n",
      "[Batch #25500/122727] Loss: 1356.3812956809998\n",
      "[Batch #26000/122727] Loss: 1361.572193622589\n",
      "[Batch #26500/122727] Loss: 1368.218424797058\n",
      "[Batch #27000/122727] Loss: 1367.0509893894196\n",
      "[Batch #27500/122727] Loss: 1359.0939240455627\n",
      "[Batch #28000/122727] Loss: 1363.7721018791199\n",
      "[Batch #28500/122727] Loss: 1358.0794349908829\n",
      "[Batch #29000/122727] Loss: 1369.0083448886871\n",
      "[Batch #29500/122727] Loss: 1364.5306024551392\n",
      "[Batch #30000/122727] Loss: 1375.2414622306824\n",
      "[Batch #30500/122727] Loss: 1363.1878678798676\n",
      "[Batch #31000/122727] Loss: 1371.5967936515808\n",
      "[Batch #31500/122727] Loss: 1373.830044746399\n",
      "[Batch #32000/122727] Loss: 1353.8955824375153\n",
      "[Batch #32500/122727] Loss: 1353.1455583572388\n",
      "[Batch #33000/122727] Loss: 1365.9805669784546\n",
      "[Batch #33500/122727] Loss: 1368.7933497428894\n",
      "[Batch #34000/122727] Loss: 1367.297049999237\n",
      "[Batch #34500/122727] Loss: 1361.2888405323029\n",
      "[Batch #35000/122727] Loss: 1356.5827560424805\n",
      "[Batch #35500/122727] Loss: 1364.5481748580933\n",
      "[Batch #36000/122727] Loss: 1370.4966142177582\n",
      "[Batch #36500/122727] Loss: 1357.618005156517\n",
      "[Batch #37000/122727] Loss: 1363.5376068353653\n",
      "[Batch #37500/122727] Loss: 1367.9489297866821\n",
      "[Batch #38000/122727] Loss: 1370.7215292453766\n",
      "[Batch #38500/122727] Loss: 1368.8617193698883\n",
      "[Batch #39000/122727] Loss: 1359.287618637085\n",
      "[Batch #39500/122727] Loss: 1363.9150047302246\n",
      "[Batch #40000/122727] Loss: 1370.0801281929016\n",
      "[Batch #40500/122727] Loss: 1360.8680279254913\n",
      "[Batch #41000/122727] Loss: 1357.5532495975494\n",
      "[Batch #41500/122727] Loss: 1357.973007440567\n",
      "[Batch #42000/122727] Loss: 1374.79101395607\n",
      "[Batch #42500/122727] Loss: 1361.5397844314575\n",
      "[Batch #43000/122727] Loss: 1365.3902678489685\n",
      "[Batch #43500/122727] Loss: 1366.2254483699799\n",
      "[Batch #44000/122727] Loss: 1363.9127011299133\n",
      "[Batch #44500/122727] Loss: 1365.5598583221436\n",
      "[Batch #45000/122727] Loss: 1367.6557977199554\n",
      "[Batch #45500/122727] Loss: 1366.9408720731735\n",
      "[Batch #46000/122727] Loss: 1362.5133883953094\n",
      "[Batch #46500/122727] Loss: 1369.0567078590393\n",
      "[Batch #47000/122727] Loss: 1368.3629430532455\n",
      "[Batch #47500/122727] Loss: 1367.659384727478\n",
      "[Batch #48000/122727] Loss: 1362.7666883468628\n",
      "[Batch #48500/122727] Loss: 1367.1756172180176\n",
      "[Batch #49000/122727] Loss: 1358.9648959636688\n",
      "[Batch #49500/122727] Loss: 1367.5380864143372\n",
      "[Batch #50000/122727] Loss: 1358.5276699066162\n",
      "[Batch #50500/122727] Loss: 1350.9554226398468\n",
      "[Batch #51000/122727] Loss: 1361.9747650623322\n",
      "[Batch #51500/122727] Loss: 1359.305665254593\n",
      "[Batch #52000/122727] Loss: 1373.8351402282715\n",
      "[Batch #52500/122727] Loss: 1365.8475317955017\n",
      "[Batch #53000/122727] Loss: 1356.8128821849823\n",
      "[Batch #53500/122727] Loss: 1366.2482793331146\n",
      "[Batch #54000/122727] Loss: 1357.2817270755768\n",
      "[Batch #54500/122727] Loss: 1367.0214936733246\n",
      "[Batch #55000/122727] Loss: 1363.087087392807\n",
      "[Batch #55500/122727] Loss: 1363.7396240234375\n",
      "[Batch #56000/122727] Loss: 1359.5684369802475\n",
      "[Batch #56500/122727] Loss: 1361.0097451210022\n",
      "[Batch #57000/122727] Loss: 1357.9078481197357\n",
      "[Batch #57500/122727] Loss: 1356.788677930832\n",
      "[Batch #58000/122727] Loss: 1361.0377304553986\n",
      "[Batch #58500/122727] Loss: 1369.6610136032104\n",
      "[Batch #59000/122727] Loss: 1358.7017667293549\n",
      "[Batch #59500/122727] Loss: 1359.8842487335205\n",
      "[Batch #60000/122727] Loss: 1363.2379341125488\n",
      "[Batch #60500/122727] Loss: 1369.3336491584778\n",
      "[Batch #61000/122727] Loss: 1362.4223084449768\n",
      "[Batch #61500/122727] Loss: 1367.0895104408264\n",
      "[Batch #62000/122727] Loss: 1362.1880958080292\n",
      "[Batch #62500/122727] Loss: 1361.728802204132\n",
      "[Batch #63000/122727] Loss: 1363.730905532837\n",
      "[Batch #63500/122727] Loss: 1356.0582220554352\n",
      "[Batch #64000/122727] Loss: 1357.2293817996979\n",
      "[Batch #64500/122727] Loss: 1360.0190951824188\n",
      "[Batch #65000/122727] Loss: 1368.5453021526337\n",
      "[Batch #65500/122727] Loss: 1361.2053670883179\n",
      "[Batch #66000/122727] Loss: 1355.644441127777\n",
      "[Batch #66500/122727] Loss: 1365.449562549591\n",
      "[Batch #67000/122727] Loss: 1369.2961964607239\n",
      "[Batch #67500/122727] Loss: 1358.3247599601746\n",
      "[Batch #68000/122727] Loss: 1362.5869581699371\n",
      "[Batch #68500/122727] Loss: 1365.722534418106\n",
      "[Batch #69000/122727] Loss: 1363.5979979038239\n",
      "[Batch #69500/122727] Loss: 1364.2830855846405\n",
      "[Batch #70000/122727] Loss: 1360.5761350393295\n",
      "[Batch #70500/122727] Loss: 1358.3204473257065\n",
      "[Batch #71000/122727] Loss: 1360.6331589221954\n",
      "[Batch #71500/122727] Loss: 1352.95005941391\n",
      "[Batch #72000/122727] Loss: 1364.9185264110565\n",
      "[Batch #72500/122727] Loss: 1362.0567969083786\n",
      "[Batch #73000/122727] Loss: 1363.6281479597092\n",
      "[Batch #73500/122727] Loss: 1374.4271221160889\n",
      "[Batch #74000/122727] Loss: 1366.9857199192047\n",
      "[Batch #74500/122727] Loss: 1359.5439171791077\n",
      "[Batch #75000/122727] Loss: 1356.0717215538025\n",
      "[Batch #75500/122727] Loss: 1363.7827503681183\n",
      "[Batch #76000/122727] Loss: 1370.1280224323273\n",
      "[Batch #76500/122727] Loss: 1357.488478899002\n",
      "[Batch #77000/122727] Loss: 1359.1832438707352\n",
      "[Batch #77500/122727] Loss: 1373.517433166504\n",
      "[Batch #78000/122727] Loss: 1357.212458372116\n",
      "[Batch #78500/122727] Loss: 1360.2986929416656\n",
      "[Batch #79000/122727] Loss: 1358.8902876377106\n",
      "[Batch #79500/122727] Loss: 1360.0271980762482\n",
      "[Batch #80000/122727] Loss: 1366.6465678215027\n",
      "[Batch #80500/122727] Loss: 1359.3084008693695\n",
      "[Batch #81000/122727] Loss: 1362.7788546085358\n",
      "[Batch #81500/122727] Loss: 1368.816066980362\n",
      "[Batch #82000/122727] Loss: 1363.8587205410004\n",
      "[Batch #82500/122727] Loss: 1362.2530310153961\n",
      "[Batch #83000/122727] Loss: 1358.9722700119019\n",
      "[Batch #83500/122727] Loss: 1363.4752531051636\n",
      "[Batch #84000/122727] Loss: 1360.2731232643127\n",
      "[Batch #84500/122727] Loss: 1367.0059087276459\n",
      "[Batch #85000/122727] Loss: 1367.0445778369904\n",
      "[Batch #85500/122727] Loss: 1356.9515602588654\n",
      "[Batch #86000/122727] Loss: 1377.9073808193207\n",
      "[Batch #86500/122727] Loss: 1363.135073184967\n",
      "[Batch #87000/122727] Loss: 1362.4557945728302\n",
      "[Batch #87500/122727] Loss: 1365.3999333381653\n",
      "[Batch #88000/122727] Loss: 1367.214037179947\n",
      "[Batch #88500/122727] Loss: 1368.5689613819122\n",
      "[Batch #89000/122727] Loss: 1357.200356721878\n",
      "[Batch #89500/122727] Loss: 1360.986884355545\n",
      "[Batch #90000/122727] Loss: 1368.8743062019348\n",
      "[Batch #90500/122727] Loss: 1364.6187918186188\n",
      "[Batch #91000/122727] Loss: 1363.5636522769928\n",
      "[Batch #91500/122727] Loss: 1359.7904599905014\n",
      "[Batch #92000/122727] Loss: 1366.8814997673035\n",
      "[Batch #92500/122727] Loss: 1355.0682318210602\n",
      "[Batch #93000/122727] Loss: 1363.9092347621918\n",
      "[Batch #93500/122727] Loss: 1365.9498488903046\n",
      "[Batch #94000/122727] Loss: 1348.6704466342926\n",
      "[Batch #94500/122727] Loss: 1355.1972398757935\n",
      "[Batch #95000/122727] Loss: 1366.4902002811432\n",
      "[Batch #95500/122727] Loss: 1361.1936964988708\n",
      "[Batch #96000/122727] Loss: 1365.365835905075\n",
      "[Batch #96500/122727] Loss: 1359.2336585521698\n",
      "[Batch #97000/122727] Loss: 1356.2125532627106\n",
      "[Batch #97500/122727] Loss: 1361.2935254573822\n",
      "[Batch #98000/122727] Loss: 1362.3249397277832\n",
      "[Batch #98500/122727] Loss: 1363.400672674179\n",
      "[Batch #99000/122727] Loss: 1355.7599289417267\n",
      "[Batch #99500/122727] Loss: 1362.1047852039337\n",
      "[Batch #100000/122727] Loss: 1357.8455379009247\n",
      "[Batch #100500/122727] Loss: 1355.1193418502808\n",
      "[Batch #101000/122727] Loss: 1365.9347348213196\n",
      "[Batch #101500/122727] Loss: 1361.3058722019196\n",
      "[Batch #102000/122727] Loss: 1359.9576303958893\n",
      "[Batch #102500/122727] Loss: 1364.5201454162598\n",
      "[Batch #103000/122727] Loss: 1363.6001543998718\n",
      "[Batch #103500/122727] Loss: 1366.8042719364166\n",
      "[Batch #104000/122727] Loss: 1360.3780236244202\n",
      "[Batch #104500/122727] Loss: 1361.9792857170105\n",
      "[Batch #105000/122727] Loss: 1355.7056418657303\n",
      "[Batch #105500/122727] Loss: 1363.4202142953873\n",
      "[Batch #106000/122727] Loss: 1358.385751247406\n",
      "[Batch #106500/122727] Loss: 1367.3906507492065\n",
      "[Batch #107000/122727] Loss: 1359.2249777317047\n",
      "[Batch #107500/122727] Loss: 1360.4871270656586\n",
      "[Batch #108000/122727] Loss: 1355.4290137290955\n",
      "[Batch #108500/122727] Loss: 1367.7235643863678\n",
      "[Batch #109000/122727] Loss: 1365.9339499473572\n",
      "[Batch #109500/122727] Loss: 1366.191445350647\n",
      "[Batch #110000/122727] Loss: 1361.7860069274902\n",
      "[Batch #110500/122727] Loss: 1362.6220335960388\n",
      "[Batch #111000/122727] Loss: 1361.4935846328735\n",
      "[Batch #111500/122727] Loss: 1365.910007238388\n",
      "[Batch #112000/122727] Loss: 1363.5981261730194\n",
      "[Batch #112500/122727] Loss: 1358.490117073059\n",
      "[Batch #113000/122727] Loss: 1367.915484905243\n",
      "[Batch #113500/122727] Loss: 1360.1932361125946\n",
      "[Batch #114000/122727] Loss: 1361.0372989177704\n",
      "[Batch #114500/122727] Loss: 1368.7898919582367\n",
      "[Batch #115000/122727] Loss: 1360.39972615242\n",
      "[Batch #115500/122727] Loss: 1368.8678529262543\n",
      "[Batch #116000/122727] Loss: 1360.907615184784\n",
      "[Batch #116500/122727] Loss: 1366.8965402841568\n",
      "[Batch #117000/122727] Loss: 1361.9203610420227\n",
      "[Batch #117500/122727] Loss: 1361.3878180980682\n",
      "[Batch #118000/122727] Loss: 1361.752554655075\n",
      "[Batch #118500/122727] Loss: 1361.964135169983\n",
      "[Batch #119000/122727] Loss: 1368.012395620346\n",
      "[Batch #119500/122727] Loss: 1366.4242496490479\n",
      "[Batch #120000/122727] Loss: 1358.828283071518\n",
      "[Batch #120500/122727] Loss: 1360.170289516449\n",
      "[Batch #121000/122727] Loss: 1361.9964978694916\n",
      "[Batch #121500/122727] Loss: 1347.153405547142\n",
      "[Batch #122000/122727] Loss: 1363.03160572052\n",
      "[Batch #122500/122727] Loss: 1353.2146866321564\n",
      "\n",
      "Validating model...\n",
      "Epoch 4 complete. Training loss: 2.7255893714253476 - Validation loss: 2.508064910738731\n",
      "----- END OF EPOCH 4 -----\n",
      "\n",
      "----- STARTING EPOCH 5 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1351.8479863405228\n",
      "[Batch #1000/122727] Loss: 1353.33034157753\n",
      "[Batch #1500/122727] Loss: 1354.5878994464874\n",
      "[Batch #2000/122727] Loss: 1367.5549895763397\n",
      "[Batch #2500/122727] Loss: 1357.4413208961487\n",
      "[Batch #3000/122727] Loss: 1367.4341719150543\n",
      "[Batch #3500/122727] Loss: 1367.6995072364807\n",
      "[Batch #4000/122727] Loss: 1361.6467604637146\n",
      "[Batch #4500/122727] Loss: 1358.8303879499435\n",
      "[Batch #5000/122727] Loss: 1365.5791795253754\n",
      "[Batch #5500/122727] Loss: 1356.9302353858948\n",
      "[Batch #6000/122727] Loss: 1362.8531584739685\n",
      "[Batch #6500/122727] Loss: 1362.5859501361847\n",
      "[Batch #7000/122727] Loss: 1364.0444889068604\n",
      "[Batch #7500/122727] Loss: 1367.088660478592\n",
      "[Batch #8000/122727] Loss: 1349.9006087779999\n",
      "[Batch #8500/122727] Loss: 1361.5597014427185\n",
      "[Batch #9000/122727] Loss: 1364.080218553543\n",
      "[Batch #9500/122727] Loss: 1366.7080917358398\n",
      "[Batch #10000/122727] Loss: 1372.0073971748352\n",
      "[Batch #10500/122727] Loss: 1364.0530101060867\n",
      "[Batch #11000/122727] Loss: 1364.4102754592896\n",
      "[Batch #11500/122727] Loss: 1370.4322016239166\n",
      "[Batch #12000/122727] Loss: 1369.1466300487518\n",
      "[Batch #12500/122727] Loss: 1363.7352859973907\n",
      "[Batch #13000/122727] Loss: 1372.5846457481384\n",
      "[Batch #13500/122727] Loss: 1350.8883078098297\n",
      "[Batch #14000/122727] Loss: 1374.921060204506\n",
      "[Batch #14500/122727] Loss: 1365.6277301311493\n",
      "[Batch #15000/122727] Loss: 1364.6151411533356\n",
      "[Batch #15500/122727] Loss: 1356.5319919586182\n",
      "[Batch #16000/122727] Loss: 1360.6891779899597\n",
      "[Batch #16500/122727] Loss: 1358.2016534805298\n",
      "[Batch #17000/122727] Loss: 1365.6089293956757\n",
      "[Batch #17500/122727] Loss: 1356.2647173404694\n",
      "[Batch #18000/122727] Loss: 1373.2908861637115\n",
      "[Batch #18500/122727] Loss: 1372.999429345131\n",
      "[Batch #19000/122727] Loss: 1372.338220357895\n",
      "[Batch #19500/122727] Loss: 1355.6731057167053\n",
      "[Batch #20000/122727] Loss: 1363.895606994629\n",
      "[Batch #20500/122727] Loss: 1361.512335062027\n",
      "[Batch #21000/122727] Loss: 1365.659500360489\n",
      "[Batch #21500/122727] Loss: 1360.0818049907684\n",
      "[Batch #22000/122727] Loss: 1353.859700679779\n",
      "[Batch #22500/122727] Loss: 1362.2253127098083\n",
      "[Batch #23000/122727] Loss: 1362.4707503318787\n",
      "[Batch #23500/122727] Loss: 1357.8188741207123\n",
      "[Batch #24000/122727] Loss: 1353.8709151744843\n",
      "[Batch #24500/122727] Loss: 1350.5430488586426\n",
      "[Batch #25000/122727] Loss: 1366.8963994979858\n",
      "[Batch #25500/122727] Loss: 1368.7470228672028\n",
      "[Batch #26000/122727] Loss: 1359.3450541496277\n",
      "[Batch #26500/122727] Loss: 1362.1519763469696\n",
      "[Batch #27000/122727] Loss: 1365.289707183838\n",
      "[Batch #27500/122727] Loss: 1359.7215765714645\n",
      "[Batch #28000/122727] Loss: 1361.3789498806\n",
      "[Batch #28500/122727] Loss: 1354.2793083190918\n",
      "[Batch #29000/122727] Loss: 1364.3743121623993\n",
      "[Batch #29500/122727] Loss: 1366.0091412067413\n",
      "[Batch #30000/122727] Loss: 1360.163247346878\n",
      "[Batch #30500/122727] Loss: 1356.21360039711\n",
      "[Batch #31000/122727] Loss: 1348.1201448440552\n",
      "[Batch #31500/122727] Loss: 1355.0285992622375\n",
      "[Batch #32000/122727] Loss: 1359.8203384876251\n",
      "[Batch #32500/122727] Loss: 1371.6021475791931\n",
      "[Batch #33000/122727] Loss: 1366.263340473175\n",
      "[Batch #33500/122727] Loss: 1360.0965243577957\n",
      "[Batch #34000/122727] Loss: 1368.2901833057404\n",
      "[Batch #34500/122727] Loss: 1369.2088861465454\n",
      "[Batch #35000/122727] Loss: 1353.258605480194\n",
      "[Batch #35500/122727] Loss: 1362.0288345813751\n",
      "[Batch #36000/122727] Loss: 1362.0885426998138\n",
      "[Batch #36500/122727] Loss: 1367.7593467235565\n",
      "[Batch #37000/122727] Loss: 1362.0058085918427\n",
      "[Batch #37500/122727] Loss: 1356.47203707695\n",
      "[Batch #38000/122727] Loss: 1365.0097472667694\n",
      "[Batch #38500/122727] Loss: 1369.603014945984\n",
      "[Batch #39000/122727] Loss: 1363.065211057663\n",
      "[Batch #39500/122727] Loss: 1357.8609068393707\n",
      "[Batch #40000/122727] Loss: 1357.5587749481201\n",
      "[Batch #40500/122727] Loss: 1354.950012922287\n",
      "[Batch #41000/122727] Loss: 1368.2480931282043\n",
      "[Batch #41500/122727] Loss: 1356.7265462875366\n",
      "[Batch #42000/122727] Loss: 1355.7598600387573\n",
      "[Batch #42500/122727] Loss: 1358.7173748016357\n",
      "[Batch #43000/122727] Loss: 1363.4433215856552\n",
      "[Batch #43500/122727] Loss: 1365.6998083591461\n",
      "[Batch #44000/122727] Loss: 1366.1980826854706\n",
      "[Batch #44500/122727] Loss: 1364.0091700553894\n",
      "[Batch #45000/122727] Loss: 1377.297093629837\n",
      "[Batch #45500/122727] Loss: 1358.6083858013153\n",
      "[Batch #46000/122727] Loss: 1366.818369626999\n",
      "[Batch #46500/122727] Loss: 1366.230640411377\n",
      "[Batch #47000/122727] Loss: 1358.7565298080444\n",
      "[Batch #47500/122727] Loss: 1361.8236117362976\n",
      "[Batch #48000/122727] Loss: 1364.2049853801727\n",
      "[Batch #48500/122727] Loss: 1352.50845682621\n",
      "[Batch #49000/122727] Loss: 1352.6601178646088\n",
      "[Batch #49500/122727] Loss: 1362.6114208698273\n",
      "[Batch #50000/122727] Loss: 1363.8183798789978\n",
      "[Batch #50500/122727] Loss: 1355.809645652771\n",
      "[Batch #51000/122727] Loss: 1353.0992078781128\n",
      "[Batch #51500/122727] Loss: 1365.9493250846863\n",
      "[Batch #52000/122727] Loss: 1362.5152316093445\n",
      "[Batch #52500/122727] Loss: 1361.0400040149689\n",
      "[Batch #53000/122727] Loss: 1367.5843524932861\n",
      "[Batch #53500/122727] Loss: 1366.3277380466461\n",
      "[Batch #54000/122727] Loss: 1355.8156774044037\n",
      "[Batch #54500/122727] Loss: 1352.1112877130508\n",
      "[Batch #55000/122727] Loss: 1358.5999433994293\n",
      "[Batch #55500/122727] Loss: 1361.4265501499176\n",
      "[Batch #56000/122727] Loss: 1356.108397245407\n",
      "[Batch #56500/122727] Loss: 1366.1481602191925\n",
      "[Batch #57000/122727] Loss: 1359.5249812602997\n",
      "[Batch #57500/122727] Loss: 1361.844219326973\n",
      "[Batch #58000/122727] Loss: 1368.4635360240936\n",
      "[Batch #58500/122727] Loss: 1352.9474301338196\n",
      "[Batch #59000/122727] Loss: 1360.3548274040222\n",
      "[Batch #59500/122727] Loss: 1377.8275705575943\n",
      "[Batch #60000/122727] Loss: 1358.8938584327698\n",
      "[Batch #60500/122727] Loss: 1361.0509860515594\n",
      "[Batch #61000/122727] Loss: 1360.7676508426666\n",
      "[Batch #61500/122727] Loss: 1359.552167415619\n",
      "[Batch #62000/122727] Loss: 1364.8023462295532\n",
      "[Batch #62500/122727] Loss: 1364.4609429836273\n",
      "[Batch #63000/122727] Loss: 1357.9469065666199\n",
      "[Batch #63500/122727] Loss: 1363.6069707870483\n",
      "[Batch #64000/122727] Loss: 1364.988365650177\n",
      "[Batch #64500/122727] Loss: 1359.8304789066315\n",
      "[Batch #65000/122727] Loss: 1360.4027092456818\n",
      "[Batch #65500/122727] Loss: 1367.1119256019592\n",
      "[Batch #66000/122727] Loss: 1362.770417690277\n",
      "[Batch #66500/122727] Loss: 1368.141313791275\n",
      "[Batch #67000/122727] Loss: 1359.7729219198227\n",
      "[Batch #67500/122727] Loss: 1366.60396027565\n",
      "[Batch #68000/122727] Loss: 1363.0303962230682\n",
      "[Batch #68500/122727] Loss: 1355.467712879181\n",
      "[Batch #69000/122727] Loss: 1360.9218709468842\n",
      "[Batch #69500/122727] Loss: 1359.2168221473694\n",
      "[Batch #70000/122727] Loss: 1367.249454498291\n",
      "[Batch #70500/122727] Loss: 1368.5928170681\n",
      "[Batch #71000/122727] Loss: 1356.9555518627167\n",
      "[Batch #71500/122727] Loss: 1359.010755777359\n",
      "[Batch #72000/122727] Loss: 1369.585389137268\n",
      "[Batch #72500/122727] Loss: 1367.1043503284454\n",
      "[Batch #73000/122727] Loss: 1362.4110049009323\n",
      "[Batch #73500/122727] Loss: 1353.990477323532\n",
      "[Batch #74000/122727] Loss: 1359.1763973236084\n",
      "[Batch #74500/122727] Loss: 1368.0056941509247\n",
      "[Batch #75000/122727] Loss: 1367.7765135765076\n",
      "[Batch #75500/122727] Loss: 1370.9300587177277\n",
      "[Batch #76000/122727] Loss: 1367.428855419159\n",
      "[Batch #76500/122727] Loss: 1357.8500742912292\n",
      "[Batch #77000/122727] Loss: 1368.7802996635437\n",
      "[Batch #77500/122727] Loss: 1364.4594782590866\n",
      "[Batch #78000/122727] Loss: 1360.6336815357208\n",
      "[Batch #78500/122727] Loss: 1372.7386178970337\n",
      "[Batch #79000/122727] Loss: 1354.2088651657104\n",
      "[Batch #79500/122727] Loss: 1365.7821805477142\n",
      "[Batch #80000/122727] Loss: 1348.3164477348328\n",
      "[Batch #80500/122727] Loss: 1363.2285900115967\n",
      "[Batch #81000/122727] Loss: 1355.8435312509537\n",
      "[Batch #81500/122727] Loss: 1359.269519329071\n",
      "[Batch #82000/122727] Loss: 1362.2521727085114\n",
      "[Batch #82500/122727] Loss: 1364.269718170166\n",
      "[Batch #83000/122727] Loss: 1350.0308978557587\n",
      "[Batch #83500/122727] Loss: 1360.0843250751495\n",
      "[Batch #84000/122727] Loss: 1361.969871520996\n",
      "[Batch #84500/122727] Loss: 1358.3144035339355\n",
      "[Batch #85000/122727] Loss: 1356.0019602775574\n",
      "[Batch #85500/122727] Loss: 1365.983518242836\n",
      "[Batch #86000/122727] Loss: 1348.6479470729828\n",
      "[Batch #86500/122727] Loss: 1362.1022808551788\n",
      "[Batch #87000/122727] Loss: 1354.1638355255127\n",
      "[Batch #87500/122727] Loss: 1358.9923655986786\n",
      "[Batch #88000/122727] Loss: 1361.1185748577118\n",
      "[Batch #88500/122727] Loss: 1364.3738543987274\n",
      "[Batch #89000/122727] Loss: 1357.5242874622345\n",
      "[Batch #89500/122727] Loss: 1367.4269547462463\n",
      "[Batch #90000/122727] Loss: 1349.875072002411\n",
      "[Batch #90500/122727] Loss: 1357.8183236122131\n",
      "[Batch #91000/122727] Loss: 1368.5600743293762\n",
      "[Batch #91500/122727] Loss: 1359.854698896408\n",
      "[Batch #92000/122727] Loss: 1356.283096075058\n",
      "[Batch #92500/122727] Loss: 1375.3381080627441\n",
      "[Batch #93000/122727] Loss: 1364.5484623908997\n",
      "[Batch #93500/122727] Loss: 1357.4436279535294\n",
      "[Batch #94000/122727] Loss: 1372.2242360115051\n",
      "[Batch #94500/122727] Loss: 1362.3015344142914\n",
      "[Batch #95000/122727] Loss: 1369.644896030426\n",
      "[Batch #95500/122727] Loss: 1367.5171267986298\n",
      "[Batch #96000/122727] Loss: 1369.8060717582703\n",
      "[Batch #96500/122727] Loss: 1359.864458322525\n",
      "[Batch #97000/122727] Loss: 1368.195421218872\n",
      "[Batch #97500/122727] Loss: 1364.5066409111023\n",
      "[Batch #98000/122727] Loss: 1355.697815656662\n",
      "[Batch #98500/122727] Loss: 1369.8327152729034\n",
      "[Batch #99000/122727] Loss: 1360.2810127735138\n",
      "[Batch #99500/122727] Loss: 1363.2002674341202\n",
      "[Batch #100000/122727] Loss: 1363.9763207435608\n",
      "[Batch #100500/122727] Loss: 1362.2250258922577\n",
      "[Batch #101000/122727] Loss: 1355.4533903598785\n",
      "[Batch #101500/122727] Loss: 1358.727544784546\n",
      "[Batch #102000/122727] Loss: 1361.8484516143799\n",
      "[Batch #102500/122727] Loss: 1355.4862797260284\n",
      "[Batch #103000/122727] Loss: 1364.5017580986023\n",
      "[Batch #103500/122727] Loss: 1366.2297703027725\n",
      "[Batch #104000/122727] Loss: 1349.7051203250885\n",
      "[Batch #104500/122727] Loss: 1362.4862534999847\n",
      "[Batch #105000/122727] Loss: 1346.9461658000946\n",
      "[Batch #105500/122727] Loss: 1367.8204250335693\n",
      "[Batch #106000/122727] Loss: 1364.9203469753265\n",
      "[Batch #106500/122727] Loss: 1360.2616510391235\n",
      "[Batch #107000/122727] Loss: 1360.7970213890076\n",
      "[Batch #107500/122727] Loss: 1365.5487236976624\n",
      "[Batch #108000/122727] Loss: 1359.8818910121918\n",
      "[Batch #108500/122727] Loss: 1357.2968275547028\n",
      "[Batch #109000/122727] Loss: 1365.178869009018\n",
      "[Batch #109500/122727] Loss: 1360.5900826454163\n",
      "[Batch #110000/122727] Loss: 1357.83811044693\n",
      "[Batch #110500/122727] Loss: 1355.5366493463516\n",
      "[Batch #111000/122727] Loss: 1359.6124131679535\n",
      "[Batch #111500/122727] Loss: 1352.9062695503235\n",
      "[Batch #112000/122727] Loss: 1368.7834856510162\n",
      "[Batch #112500/122727] Loss: 1353.8274912834167\n",
      "[Batch #113000/122727] Loss: 1362.1868789196014\n",
      "[Batch #113500/122727] Loss: 1354.213808298111\n",
      "[Batch #114000/122727] Loss: 1356.7827212810516\n",
      "[Batch #114500/122727] Loss: 1366.6837763786316\n",
      "[Batch #115000/122727] Loss: 1359.9641218185425\n",
      "[Batch #115500/122727] Loss: 1359.8211965560913\n",
      "[Batch #116000/122727] Loss: 1360.6000938415527\n",
      "[Batch #116500/122727] Loss: 1359.5977654457092\n",
      "[Batch #117000/122727] Loss: 1363.294226884842\n",
      "[Batch #117500/122727] Loss: 1364.7556908130646\n",
      "[Batch #118000/122727] Loss: 1365.34428524971\n",
      "[Batch #118500/122727] Loss: 1364.299677848816\n",
      "[Batch #119000/122727] Loss: 1366.8451704978943\n",
      "[Batch #119500/122727] Loss: 1358.076905965805\n",
      "[Batch #120000/122727] Loss: 1366.2380023002625\n",
      "[Batch #120500/122727] Loss: 1365.4638469219208\n",
      "[Batch #121000/122727] Loss: 1361.1827300786972\n",
      "[Batch #121500/122727] Loss: 1356.3011779785156\n",
      "[Batch #122000/122727] Loss: 1362.4044744968414\n",
      "[Batch #122500/122727] Loss: 1366.4541282653809\n",
      "\n",
      "Validating model...\n",
      "Epoch 5 complete. Training loss: 2.7236720929686693 - Validation loss: 2.4864258426898576\n",
      "----- END OF EPOCH 5 -----\n",
      "\n",
      "New best model found. Saving...\n",
      "----- STARTING EPOCH 6 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1367.852848291397\n",
      "[Batch #1000/122727] Loss: 1354.0911252498627\n",
      "[Batch #1500/122727] Loss: 1362.6397223472595\n",
      "[Batch #2000/122727] Loss: 1358.6391260623932\n",
      "[Batch #2500/122727] Loss: 1363.0383994579315\n",
      "[Batch #3000/122727] Loss: 1360.5665535926819\n",
      "[Batch #3500/122727] Loss: 1367.2040954828262\n",
      "[Batch #4000/122727] Loss: 1368.412764787674\n",
      "[Batch #4500/122727] Loss: 1365.5802711248398\n",
      "[Batch #5000/122727] Loss: 1367.0756566524506\n",
      "[Batch #5500/122727] Loss: 1360.1184455156326\n",
      "[Batch #6000/122727] Loss: 1358.2317625284195\n",
      "[Batch #6500/122727] Loss: 1364.5329110622406\n",
      "[Batch #7000/122727] Loss: 1367.613434791565\n",
      "[Batch #7500/122727] Loss: 1367.217181444168\n",
      "[Batch #8000/122727] Loss: 1356.9340090751648\n",
      "[Batch #8500/122727] Loss: 1351.9092795848846\n",
      "[Batch #9000/122727] Loss: 1360.7420508861542\n",
      "[Batch #9500/122727] Loss: 1355.6006886959076\n",
      "[Batch #10000/122727] Loss: 1368.643773317337\n",
      "[Batch #10500/122727] Loss: 1356.437326669693\n",
      "[Batch #11000/122727] Loss: 1360.1145853996277\n",
      "[Batch #11500/122727] Loss: 1361.1010463237762\n",
      "[Batch #12000/122727] Loss: 1357.9190044403076\n",
      "[Batch #12500/122727] Loss: 1372.62064909935\n",
      "[Batch #13000/122727] Loss: 1364.944489479065\n",
      "[Batch #13500/122727] Loss: 1363.0771317481995\n",
      "[Batch #14000/122727] Loss: 1364.010704278946\n",
      "[Batch #14500/122727] Loss: 1352.242092847824\n",
      "[Batch #15000/122727] Loss: 1360.7213411331177\n",
      "[Batch #15500/122727] Loss: 1360.3374876976013\n",
      "[Batch #16000/122727] Loss: 1356.1685392856598\n",
      "[Batch #16500/122727] Loss: 1363.3498694896698\n",
      "[Batch #17000/122727] Loss: 1358.7286899089813\n",
      "[Batch #17500/122727] Loss: 1367.14288854599\n",
      "[Batch #18000/122727] Loss: 1359.6932265758514\n",
      "[Batch #18500/122727] Loss: 1355.367163658142\n",
      "[Batch #19000/122727] Loss: 1358.5114586353302\n",
      "[Batch #19500/122727] Loss: 1356.8650469779968\n",
      "[Batch #20000/122727] Loss: 1359.4229011535645\n",
      "[Batch #20500/122727] Loss: 1363.0054647922516\n",
      "[Batch #21000/122727] Loss: 1359.3324568271637\n",
      "[Batch #21500/122727] Loss: 1366.217200756073\n",
      "[Batch #22000/122727] Loss: 1360.7234840393066\n",
      "[Batch #22500/122727] Loss: 1364.8447964191437\n",
      "[Batch #23000/122727] Loss: 1357.8390095233917\n",
      "[Batch #23500/122727] Loss: 1363.3239915370941\n",
      "[Batch #24000/122727] Loss: 1368.3206326961517\n",
      "[Batch #24500/122727] Loss: 1357.8770627975464\n",
      "[Batch #25000/122727] Loss: 1357.3973207473755\n",
      "[Batch #25500/122727] Loss: 1367.7203985452652\n",
      "[Batch #26000/122727] Loss: 1367.3349192142487\n",
      "[Batch #26500/122727] Loss: 1361.7732501029968\n",
      "[Batch #27000/122727] Loss: 1360.5367918014526\n",
      "[Batch #27500/122727] Loss: 1362.784679055214\n",
      "[Batch #28000/122727] Loss: 1355.2032308578491\n",
      "[Batch #28500/122727] Loss: 1364.2273041009903\n",
      "[Batch #29000/122727] Loss: 1354.5846917629242\n",
      "[Batch #29500/122727] Loss: 1363.3775358200073\n",
      "[Batch #30000/122727] Loss: 1362.7872552871704\n",
      "[Batch #30500/122727] Loss: 1359.5089700222015\n",
      "[Batch #31000/122727] Loss: 1357.908756017685\n",
      "[Batch #31500/122727] Loss: 1364.2122192382812\n",
      "[Batch #32000/122727] Loss: 1351.8708069324493\n",
      "[Batch #32500/122727] Loss: 1362.184906244278\n",
      "[Batch #33000/122727] Loss: 1356.352399110794\n",
      "[Batch #33500/122727] Loss: 1355.6922569274902\n",
      "[Batch #34000/122727] Loss: 1364.7593607902527\n",
      "[Batch #34500/122727] Loss: 1361.0060818195343\n",
      "[Batch #35000/122727] Loss: 1363.1214179992676\n",
      "[Batch #35500/122727] Loss: 1362.2804126739502\n",
      "[Batch #36000/122727] Loss: 1362.3219091892242\n",
      "[Batch #36500/122727] Loss: 1368.7533061504364\n",
      "[Batch #37000/122727] Loss: 1365.0579855442047\n",
      "[Batch #37500/122727] Loss: 1350.4676704406738\n",
      "[Batch #38000/122727] Loss: 1363.2231709957123\n",
      "[Batch #38500/122727] Loss: 1361.5275654792786\n",
      "[Batch #39000/122727] Loss: 1366.6144788265228\n",
      "[Batch #39500/122727] Loss: 1352.4204995632172\n",
      "[Batch #40000/122727] Loss: 1354.5479295253754\n",
      "[Batch #40500/122727] Loss: 1363.6907637119293\n",
      "[Batch #41000/122727] Loss: 1358.438901424408\n",
      "[Batch #41500/122727] Loss: 1360.1074237823486\n",
      "[Batch #42000/122727] Loss: 1358.4220864772797\n",
      "[Batch #42500/122727] Loss: 1359.7788743972778\n",
      "[Batch #43000/122727] Loss: 1354.3353389501572\n",
      "[Batch #43500/122727] Loss: 1358.8446321487427\n",
      "[Batch #44000/122727] Loss: 1352.3360452651978\n",
      "[Batch #44500/122727] Loss: 1357.3815405368805\n",
      "[Batch #45000/122727] Loss: 1361.230224609375\n",
      "[Batch #45500/122727] Loss: 1360.9062556028366\n",
      "[Batch #46000/122727] Loss: 1361.965974330902\n",
      "[Batch #46500/122727] Loss: 1364.568192601204\n",
      "[Batch #47000/122727] Loss: 1360.2725052833557\n",
      "[Batch #47500/122727] Loss: 1365.4498374462128\n",
      "[Batch #48000/122727] Loss: 1362.5731279850006\n",
      "[Batch #48500/122727] Loss: 1368.2389419078827\n",
      "[Batch #49000/122727] Loss: 1366.0959300994873\n",
      "[Batch #49500/122727] Loss: 1362.398155927658\n",
      "[Batch #50000/122727] Loss: 1358.1502821445465\n",
      "[Batch #50500/122727] Loss: 1361.0013501644135\n",
      "[Batch #51000/122727] Loss: 1366.1422990560532\n",
      "[Batch #51500/122727] Loss: 1367.476214170456\n",
      "[Batch #52000/122727] Loss: 1364.3762166500092\n",
      "[Batch #52500/122727] Loss: 1358.9682705402374\n",
      "[Batch #53000/122727] Loss: 1360.5336066484451\n",
      "[Batch #53500/122727] Loss: 1356.8061618804932\n",
      "[Batch #54000/122727] Loss: 1356.021965265274\n",
      "[Batch #54500/122727] Loss: 1359.929958820343\n",
      "[Batch #55000/122727] Loss: 1367.6166718006134\n",
      "[Batch #55500/122727] Loss: 1362.7109196186066\n",
      "[Batch #56000/122727] Loss: 1365.5574038028717\n",
      "[Batch #56500/122727] Loss: 1370.3935062885284\n",
      "[Batch #57000/122727] Loss: 1358.3036358356476\n",
      "[Batch #57500/122727] Loss: 1366.6229312419891\n",
      "[Batch #58000/122727] Loss: 1366.7296998500824\n",
      "[Batch #58500/122727] Loss: 1365.6376345157623\n",
      "[Batch #59000/122727] Loss: 1353.8581206798553\n",
      "[Batch #59500/122727] Loss: 1364.8049252033234\n",
      "[Batch #60000/122727] Loss: 1364.8805553913116\n",
      "[Batch #60500/122727] Loss: 1366.495735168457\n",
      "[Batch #61000/122727] Loss: 1356.1312787532806\n",
      "[Batch #61500/122727] Loss: 1366.6791009902954\n",
      "[Batch #62000/122727] Loss: 1360.9489703178406\n",
      "[Batch #62500/122727] Loss: 1359.3036303520203\n",
      "[Batch #63000/122727] Loss: 1358.0871732234955\n",
      "[Batch #63500/122727] Loss: 1361.8815298080444\n",
      "[Batch #64000/122727] Loss: 1360.2047219276428\n",
      "[Batch #64500/122727] Loss: 1359.8628432750702\n",
      "[Batch #65000/122727] Loss: 1360.6071639060974\n",
      "[Batch #65500/122727] Loss: 1355.450654745102\n",
      "[Batch #66000/122727] Loss: 1360.2766642570496\n",
      "[Batch #66500/122727] Loss: 1365.2546451091766\n",
      "[Batch #67000/122727] Loss: 1359.389211177826\n",
      "[Batch #67500/122727] Loss: 1349.318038225174\n",
      "[Batch #68000/122727] Loss: 1363.2272369861603\n",
      "[Batch #68500/122727] Loss: 1353.3521661758423\n",
      "[Batch #69000/122727] Loss: 1359.3883998394012\n",
      "[Batch #69500/122727] Loss: 1359.1155282258987\n",
      "[Batch #70000/122727] Loss: 1357.024820804596\n",
      "[Batch #70500/122727] Loss: 1365.9552853107452\n",
      "[Batch #71000/122727] Loss: 1371.8045816421509\n",
      "[Batch #71500/122727] Loss: 1360.2559485435486\n",
      "[Batch #72000/122727] Loss: 1351.7904665470123\n",
      "[Batch #72500/122727] Loss: 1355.9790797233582\n",
      "[Batch #73000/122727] Loss: 1364.001693725586\n",
      "[Batch #73500/122727] Loss: 1367.8016438484192\n",
      "[Batch #74000/122727] Loss: 1367.4555714130402\n",
      "[Batch #74500/122727] Loss: 1360.358613371849\n",
      "[Batch #75000/122727] Loss: 1364.936857700348\n",
      "[Batch #75500/122727] Loss: 1359.792512178421\n",
      "[Batch #76000/122727] Loss: 1358.1543114185333\n",
      "[Batch #76500/122727] Loss: 1355.2510795593262\n",
      "[Batch #77000/122727] Loss: 1359.0303852558136\n",
      "[Batch #77500/122727] Loss: 1370.5628407001495\n",
      "[Batch #78000/122727] Loss: 1358.446545600891\n",
      "[Batch #78500/122727] Loss: 1360.4342181682587\n",
      "[Batch #79000/122727] Loss: 1368.4137721061707\n",
      "[Batch #79500/122727] Loss: 1359.2523119449615\n",
      "[Batch #80000/122727] Loss: 1354.4383561611176\n",
      "[Batch #80500/122727] Loss: 1366.942221403122\n",
      "[Batch #81000/122727] Loss: 1358.6220433712006\n",
      "[Batch #81500/122727] Loss: 1354.2705030441284\n",
      "[Batch #82000/122727] Loss: 1356.6403539180756\n",
      "[Batch #82500/122727] Loss: 1367.6990506649017\n",
      "[Batch #83000/122727] Loss: 1359.053310394287\n",
      "[Batch #83500/122727] Loss: 1362.6787011623383\n",
      "[Batch #84000/122727] Loss: 1359.1662142276764\n",
      "[Batch #84500/122727] Loss: 1362.7174820899963\n",
      "[Batch #85000/122727] Loss: 1361.96533203125\n",
      "[Batch #85500/122727] Loss: 1364.6139771938324\n",
      "[Batch #86000/122727] Loss: 1358.5174355506897\n",
      "[Batch #86500/122727] Loss: 1369.05322432518\n",
      "[Batch #87000/122727] Loss: 1355.962527513504\n",
      "[Batch #87500/122727] Loss: 1360.781566143036\n",
      "[Batch #88000/122727] Loss: 1354.5210683345795\n",
      "[Batch #88500/122727] Loss: 1360.9735043048859\n",
      "[Batch #89000/122727] Loss: 1357.212200641632\n",
      "[Batch #89500/122727] Loss: 1353.1860637664795\n",
      "[Batch #90000/122727] Loss: 1359.1284465789795\n",
      "[Batch #90500/122727] Loss: 1355.061973810196\n",
      "[Batch #91000/122727] Loss: 1359.89333486557\n",
      "[Batch #91500/122727] Loss: 1358.3754045963287\n",
      "[Batch #92000/122727] Loss: 1357.9004004001617\n",
      "[Batch #92500/122727] Loss: 1363.150756597519\n",
      "[Batch #93000/122727] Loss: 1360.2938935756683\n",
      "[Batch #93500/122727] Loss: 1368.7037780284882\n",
      "[Batch #94000/122727] Loss: 1366.2103130817413\n",
      "[Batch #94500/122727] Loss: 1355.0355410575867\n",
      "[Batch #95000/122727] Loss: 1362.2157247066498\n",
      "[Batch #95500/122727] Loss: 1361.5498356819153\n",
      "[Batch #96000/122727] Loss: 1362.6159074306488\n",
      "[Batch #96500/122727] Loss: 1364.599750995636\n",
      "[Batch #97000/122727] Loss: 1360.6586465835571\n",
      "[Batch #97500/122727] Loss: 1367.185559272766\n",
      "[Batch #98000/122727] Loss: 1355.5554099082947\n",
      "[Batch #98500/122727] Loss: 1359.9105372428894\n",
      "[Batch #99000/122727] Loss: 1367.1886794567108\n",
      "[Batch #99500/122727] Loss: 1358.215690612793\n",
      "[Batch #100000/122727] Loss: 1357.7575743198395\n",
      "[Batch #100500/122727] Loss: 1354.548121213913\n",
      "[Batch #101000/122727] Loss: 1367.3854820728302\n",
      "[Batch #101500/122727] Loss: 1366.7287917137146\n",
      "[Batch #102000/122727] Loss: 1364.6896135807037\n",
      "[Batch #102500/122727] Loss: 1368.6262741088867\n",
      "[Batch #103000/122727] Loss: 1354.7169749736786\n",
      "[Batch #103500/122727] Loss: 1364.8697674274445\n",
      "[Batch #104000/122727] Loss: 1358.1280329227448\n",
      "[Batch #104500/122727] Loss: 1363.0354936122894\n",
      "[Batch #105000/122727] Loss: 1361.86416888237\n",
      "[Batch #105500/122727] Loss: 1361.6310876607895\n",
      "[Batch #106000/122727] Loss: 1362.1147108078003\n",
      "[Batch #106500/122727] Loss: 1353.6780729293823\n",
      "[Batch #107000/122727] Loss: 1361.7790639400482\n",
      "[Batch #107500/122727] Loss: 1362.3805005550385\n",
      "[Batch #108000/122727] Loss: 1361.6995317935944\n",
      "[Batch #108500/122727] Loss: 1356.8838682174683\n",
      "[Batch #109000/122727] Loss: 1366.7474837303162\n",
      "[Batch #109500/122727] Loss: 1362.2674479484558\n",
      "[Batch #110000/122727] Loss: 1363.6081938743591\n",
      "[Batch #110500/122727] Loss: 1359.0536215305328\n",
      "[Batch #111000/122727] Loss: 1362.373766899109\n",
      "[Batch #111500/122727] Loss: 1361.7408546209335\n",
      "[Batch #112000/122727] Loss: 1368.6306097507477\n",
      "[Batch #112500/122727] Loss: 1353.4318883419037\n",
      "[Batch #113000/122727] Loss: 1359.956461429596\n",
      "[Batch #113500/122727] Loss: 1355.012408733368\n",
      "[Batch #114000/122727] Loss: 1359.4859869480133\n",
      "[Batch #114500/122727] Loss: 1358.9143693447113\n",
      "[Batch #115000/122727] Loss: 1358.2728204727173\n",
      "[Batch #115500/122727] Loss: 1367.623637676239\n",
      "[Batch #116000/122727] Loss: 1366.0332698822021\n",
      "[Batch #116500/122727] Loss: 1350.86759018898\n",
      "[Batch #117000/122727] Loss: 1359.044058561325\n",
      "[Batch #117500/122727] Loss: 1357.7133328914642\n",
      "[Batch #118000/122727] Loss: 1362.9123449325562\n",
      "[Batch #118500/122727] Loss: 1367.8921550512314\n",
      "[Batch #119000/122727] Loss: 1355.9174106121063\n",
      "[Batch #119500/122727] Loss: 1357.9403891563416\n",
      "[Batch #120000/122727] Loss: 1351.2353732585907\n",
      "[Batch #120500/122727] Loss: 1364.5415847301483\n",
      "[Batch #121000/122727] Loss: 1366.1635129451752\n",
      "[Batch #121500/122727] Loss: 1357.2881914377213\n",
      "[Batch #122000/122727] Loss: 1355.6373920440674\n",
      "[Batch #122500/122727] Loss: 1361.7176671028137\n",
      "\n",
      "Validating model...\n",
      "Epoch 6 complete. Training loss: 2.7221753235650143 - Validation loss: 2.496989055251404\n",
      "----- END OF EPOCH 6 -----\n",
      "\n",
      "----- STARTING EPOCH 7 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1360.9510242938995\n",
      "[Batch #1000/122727] Loss: 1362.4701488018036\n",
      "[Batch #1500/122727] Loss: 1368.8713338375092\n",
      "[Batch #2000/122727] Loss: 1365.1791017055511\n",
      "[Batch #2500/122727] Loss: 1359.7274565696716\n",
      "[Batch #3000/122727] Loss: 1344.7795190811157\n",
      "[Batch #3500/122727] Loss: 1353.0656440258026\n",
      "[Batch #4000/122727] Loss: 1364.696619272232\n",
      "[Batch #4500/122727] Loss: 1351.2176067829132\n",
      "[Batch #5000/122727] Loss: 1351.9744908809662\n",
      "[Batch #5500/122727] Loss: 1361.1990349292755\n",
      "[Batch #6000/122727] Loss: 1365.3556934595108\n",
      "[Batch #6500/122727] Loss: 1368.6965107917786\n",
      "[Batch #7000/122727] Loss: 1364.813717365265\n",
      "[Batch #7500/122727] Loss: 1362.5141891241074\n",
      "[Batch #8000/122727] Loss: 1349.8560407161713\n",
      "[Batch #8500/122727] Loss: 1360.5459623336792\n",
      "[Batch #9000/122727] Loss: 1357.5488331317902\n",
      "[Batch #9500/122727] Loss: 1358.850783586502\n",
      "[Batch #10000/122727] Loss: 1358.691218137741\n",
      "[Batch #10500/122727] Loss: 1363.4487781524658\n",
      "[Batch #11000/122727] Loss: 1357.677695631981\n",
      "[Batch #11500/122727] Loss: 1361.8767104148865\n",
      "[Batch #12000/122727] Loss: 1351.1304948329926\n",
      "[Batch #12500/122727] Loss: 1357.6958436965942\n",
      "[Batch #13000/122727] Loss: 1360.0819163322449\n",
      "[Batch #13500/122727] Loss: 1366.9398834705353\n",
      "[Batch #14000/122727] Loss: 1358.1217865943909\n",
      "[Batch #14500/122727] Loss: 1362.5889191627502\n",
      "[Batch #15000/122727] Loss: 1360.1632075309753\n",
      "[Batch #15500/122727] Loss: 1360.363228559494\n",
      "[Batch #16000/122727] Loss: 1358.2889652252197\n",
      "[Batch #16500/122727] Loss: 1358.3648036718369\n",
      "[Batch #17000/122727] Loss: 1360.96062707901\n",
      "[Batch #17500/122727] Loss: 1350.90278506279\n",
      "[Batch #18000/122727] Loss: 1362.5578243732452\n",
      "[Batch #18500/122727] Loss: 1353.5833175182343\n",
      "[Batch #19000/122727] Loss: 1363.3779048919678\n",
      "[Batch #19500/122727] Loss: 1350.6952528953552\n",
      "[Batch #20000/122727] Loss: 1351.4968519210815\n",
      "[Batch #20500/122727] Loss: 1358.916576743126\n",
      "[Batch #21000/122727] Loss: 1360.0380206108093\n",
      "[Batch #21500/122727] Loss: 1355.8199019432068\n",
      "[Batch #22000/122727] Loss: 1356.064435005188\n",
      "[Batch #22500/122727] Loss: 1362.0958292484283\n",
      "[Batch #23000/122727] Loss: 1362.812220454216\n",
      "[Batch #23500/122727] Loss: 1363.9304370880127\n",
      "[Batch #24000/122727] Loss: 1352.3833270072937\n",
      "[Batch #24500/122727] Loss: 1368.6440496444702\n",
      "[Batch #25000/122727] Loss: 1359.8921456336975\n",
      "[Batch #25500/122727] Loss: 1359.3914482593536\n",
      "[Batch #26000/122727] Loss: 1366.6659796237946\n",
      "[Batch #26500/122727] Loss: 1350.9601347446442\n",
      "[Batch #27000/122727] Loss: 1363.2832157611847\n",
      "[Batch #27500/122727] Loss: 1354.5485953092575\n",
      "[Batch #28000/122727] Loss: 1366.0427029132843\n",
      "[Batch #28500/122727] Loss: 1357.4513499736786\n",
      "[Batch #29000/122727] Loss: 1356.9929077625275\n",
      "[Batch #29500/122727] Loss: 1360.4066132307053\n",
      "[Batch #30000/122727] Loss: 1364.2965894937515\n",
      "[Batch #30500/122727] Loss: 1357.7497231960297\n",
      "[Batch #31000/122727] Loss: 1360.804309129715\n",
      "[Batch #31500/122727] Loss: 1367.9896461963654\n",
      "[Batch #32000/122727] Loss: 1354.326247215271\n",
      "[Batch #32500/122727] Loss: 1357.8089213371277\n",
      "[Batch #33000/122727] Loss: 1364.4944427013397\n",
      "[Batch #33500/122727] Loss: 1361.0018730163574\n",
      "[Batch #34000/122727] Loss: 1355.418664932251\n",
      "[Batch #34500/122727] Loss: 1357.6692156791687\n",
      "[Batch #35000/122727] Loss: 1363.1250965595245\n",
      "[Batch #35500/122727] Loss: 1353.4457095861435\n",
      "[Batch #36000/122727] Loss: 1362.1513845920563\n",
      "[Batch #36500/122727] Loss: 1375.6341893672943\n",
      "[Batch #37000/122727] Loss: 1361.0824356079102\n",
      "[Batch #37500/122727] Loss: 1362.4468417167664\n",
      "[Batch #38000/122727] Loss: 1365.5953545570374\n",
      "[Batch #38500/122727] Loss: 1363.7581905126572\n",
      "[Batch #39000/122727] Loss: 1352.7029738426208\n",
      "[Batch #39500/122727] Loss: 1367.6572635173798\n",
      "[Batch #40000/122727] Loss: 1355.7348289489746\n",
      "[Batch #40500/122727] Loss: 1357.0116302967072\n",
      "[Batch #41000/122727] Loss: 1364.5316298007965\n",
      "[Batch #41500/122727] Loss: 1359.8039219379425\n",
      "[Batch #42000/122727] Loss: 1355.653943181038\n",
      "[Batch #42500/122727] Loss: 1356.816271662712\n",
      "[Batch #43000/122727] Loss: 1361.4351034164429\n",
      "[Batch #43500/122727] Loss: 1359.8131079673767\n",
      "[Batch #44000/122727] Loss: 1357.6959400177002\n",
      "[Batch #44500/122727] Loss: 1359.949326634407\n",
      "[Batch #45000/122727] Loss: 1361.521964788437\n",
      "[Batch #45500/122727] Loss: 1357.7940850257874\n",
      "[Batch #46000/122727] Loss: 1358.9324433803558\n",
      "[Batch #46500/122727] Loss: 1359.5614154338837\n",
      "[Batch #47000/122727] Loss: 1363.8708708286285\n",
      "[Batch #47500/122727] Loss: 1351.9661583900452\n",
      "[Batch #48000/122727] Loss: 1352.714849948883\n",
      "[Batch #48500/122727] Loss: 1365.9351341724396\n",
      "[Batch #49000/122727] Loss: 1351.3482644557953\n",
      "[Batch #49500/122727] Loss: 1354.2499330043793\n",
      "[Batch #50000/122727] Loss: 1363.4024164676666\n",
      "[Batch #50500/122727] Loss: 1362.0786983966827\n",
      "[Batch #51000/122727] Loss: 1361.6506805419922\n",
      "[Batch #51500/122727] Loss: 1361.8806037902832\n",
      "[Batch #52000/122727] Loss: 1350.5891289710999\n",
      "[Batch #52500/122727] Loss: 1360.2772369384766\n",
      "[Batch #53000/122727] Loss: 1356.710788011551\n",
      "[Batch #53500/122727] Loss: 1354.6718845367432\n",
      "[Batch #54000/122727] Loss: 1358.2861168384552\n",
      "[Batch #54500/122727] Loss: 1356.5849046707153\n",
      "[Batch #55000/122727] Loss: 1358.7914415597916\n",
      "[Batch #55500/122727] Loss: 1355.7318316698074\n",
      "[Batch #56000/122727] Loss: 1357.739844083786\n",
      "[Batch #56500/122727] Loss: 1362.55136179924\n",
      "[Batch #57000/122727] Loss: 1363.3634068965912\n",
      "[Batch #57500/122727] Loss: 1364.0732288360596\n",
      "[Batch #58000/122727] Loss: 1365.9312796592712\n",
      "[Batch #58500/122727] Loss: 1365.925142288208\n",
      "[Batch #59000/122727] Loss: 1363.3597385883331\n",
      "[Batch #59500/122727] Loss: 1365.7070677280426\n",
      "[Batch #60000/122727] Loss: 1366.1668765544891\n",
      "[Batch #60500/122727] Loss: 1357.377676486969\n",
      "[Batch #61000/122727] Loss: 1347.4737787246704\n",
      "[Batch #61500/122727] Loss: 1352.3896615505219\n",
      "[Batch #62000/122727] Loss: 1364.5347187519073\n",
      "[Batch #62500/122727] Loss: 1362.6158072948456\n",
      "[Batch #63000/122727] Loss: 1374.8569293022156\n",
      "[Batch #63500/122727] Loss: 1365.062691450119\n",
      "[Batch #64000/122727] Loss: 1358.1650488376617\n",
      "[Batch #64500/122727] Loss: 1352.3197383880615\n",
      "[Batch #65000/122727] Loss: 1356.484444618225\n",
      "[Batch #65500/122727] Loss: 1362.0139236450195\n",
      "[Batch #66000/122727] Loss: 1358.2455668449402\n",
      "[Batch #66500/122727] Loss: 1359.4640550613403\n",
      "[Batch #67000/122727] Loss: 1359.804813504219\n",
      "[Batch #67500/122727] Loss: 1351.7371037006378\n",
      "[Batch #68000/122727] Loss: 1369.0844371318817\n",
      "[Batch #68500/122727] Loss: 1352.0354282855988\n",
      "[Batch #69000/122727] Loss: 1360.4712386131287\n",
      "[Batch #69500/122727] Loss: 1359.2582740783691\n",
      "[Batch #70000/122727] Loss: 1354.5924093723297\n",
      "[Batch #70500/122727] Loss: 1367.1372411251068\n",
      "[Batch #71000/122727] Loss: 1361.3429081439972\n",
      "[Batch #71500/122727] Loss: 1355.0931305885315\n",
      "[Batch #72000/122727] Loss: 1365.0366694927216\n",
      "[Batch #72500/122727] Loss: 1357.6908584833145\n",
      "[Batch #73000/122727] Loss: 1359.3616182804108\n",
      "[Batch #73500/122727] Loss: 1363.1840122938156\n",
      "[Batch #74000/122727] Loss: 1363.156247138977\n",
      "[Batch #74500/122727] Loss: 1363.1757671833038\n",
      "[Batch #75000/122727] Loss: 1353.720980167389\n",
      "[Batch #75500/122727] Loss: 1360.4942790269852\n",
      "[Batch #76000/122727] Loss: 1356.7641055583954\n",
      "[Batch #76500/122727] Loss: 1359.8417444229126\n",
      "[Batch #77000/122727] Loss: 1359.6182978153229\n",
      "[Batch #77500/122727] Loss: 1357.2275949716568\n",
      "[Batch #78000/122727] Loss: 1347.4455361366272\n",
      "[Batch #78500/122727] Loss: 1359.574982047081\n",
      "[Batch #79000/122727] Loss: 1365.6145055294037\n",
      "[Batch #79500/122727] Loss: 1358.4436492919922\n",
      "[Batch #80000/122727] Loss: 1360.037729024887\n",
      "[Batch #80500/122727] Loss: 1356.0468802452087\n",
      "[Batch #81000/122727] Loss: 1353.7132340669632\n",
      "[Batch #81500/122727] Loss: 1353.1873354911804\n",
      "[Batch #82000/122727] Loss: 1367.890329837799\n",
      "[Batch #82500/122727] Loss: 1355.2718577384949\n",
      "[Batch #83000/122727] Loss: 1359.7531914710999\n",
      "[Batch #83500/122727] Loss: 1356.319316148758\n",
      "[Batch #84000/122727] Loss: 1354.5930905342102\n",
      "[Batch #84500/122727] Loss: 1368.3717195987701\n",
      "[Batch #85000/122727] Loss: 1355.4541079998016\n",
      "[Batch #85500/122727] Loss: 1365.563998222351\n",
      "[Batch #86000/122727] Loss: 1348.0462746620178\n",
      "[Batch #86500/122727] Loss: 1365.7294263839722\n",
      "[Batch #87000/122727] Loss: 1368.6261148452759\n",
      "[Batch #87500/122727] Loss: 1355.7627716064453\n",
      "[Batch #88000/122727] Loss: 1360.5542023181915\n",
      "[Batch #88500/122727] Loss: 1362.411369085312\n",
      "[Batch #89000/122727] Loss: 1365.4535369873047\n",
      "[Batch #89500/122727] Loss: 1353.7552344799042\n",
      "[Batch #90000/122727] Loss: 1356.7321393489838\n",
      "[Batch #90500/122727] Loss: 1366.3637118339539\n",
      "[Batch #91000/122727] Loss: 1361.295931339264\n",
      "[Batch #91500/122727] Loss: 1364.7102801799774\n",
      "[Batch #92000/122727] Loss: 1358.6806263923645\n",
      "[Batch #92500/122727] Loss: 1362.003176689148\n",
      "[Batch #93000/122727] Loss: 1364.8080985546112\n",
      "[Batch #93500/122727] Loss: 1360.8681987524033\n",
      "[Batch #94000/122727] Loss: 1369.2081801891327\n",
      "[Batch #94500/122727] Loss: 1362.6641104221344\n",
      "[Batch #95000/122727] Loss: 1357.7539672851562\n",
      "[Batch #95500/122727] Loss: 1354.0051436424255\n",
      "[Batch #96000/122727] Loss: 1362.8712553977966\n",
      "[Batch #96500/122727] Loss: 1353.519820690155\n",
      "[Batch #97000/122727] Loss: 1363.9122896194458\n",
      "[Batch #97500/122727] Loss: 1355.706663608551\n",
      "[Batch #98000/122727] Loss: 1361.0298936367035\n",
      "[Batch #98500/122727] Loss: 1365.1993336677551\n",
      "[Batch #99000/122727] Loss: 1361.1571605205536\n",
      "[Batch #99500/122727] Loss: 1367.5796117782593\n",
      "[Batch #100000/122727] Loss: 1360.7984354496002\n",
      "[Batch #100500/122727] Loss: 1360.722897529602\n",
      "[Batch #101000/122727] Loss: 1366.2962095737457\n",
      "[Batch #101500/122727] Loss: 1363.4668807983398\n",
      "[Batch #102000/122727] Loss: 1362.2093950510025\n",
      "[Batch #102500/122727] Loss: 1359.8797028064728\n",
      "[Batch #103000/122727] Loss: 1362.4404277801514\n",
      "[Batch #103500/122727] Loss: 1361.1527001857758\n",
      "[Batch #104000/122727] Loss: 1355.9779481887817\n",
      "[Batch #104500/122727] Loss: 1364.6454663276672\n",
      "[Batch #105000/122727] Loss: 1355.1883471012115\n",
      "[Batch #105500/122727] Loss: 1365.2682497501373\n",
      "[Batch #106000/122727] Loss: 1357.379683971405\n",
      "[Batch #106500/122727] Loss: 1360.8845748901367\n",
      "[Batch #107000/122727] Loss: 1356.3537130355835\n",
      "[Batch #107500/122727] Loss: 1366.3201122283936\n",
      "[Batch #108000/122727] Loss: 1365.9629011154175\n",
      "[Batch #108500/122727] Loss: 1368.7200798988342\n",
      "[Batch #109000/122727] Loss: 1364.9916709661484\n",
      "[Batch #109500/122727] Loss: 1366.5230677127838\n",
      "[Batch #110000/122727] Loss: 1365.3712487220764\n",
      "[Batch #110500/122727] Loss: 1354.4403307437897\n",
      "[Batch #111000/122727] Loss: 1359.0397372245789\n",
      "[Batch #111500/122727] Loss: 1365.2799141407013\n",
      "[Batch #112000/122727] Loss: 1358.7765204906464\n",
      "[Batch #112500/122727] Loss: 1357.6284947395325\n",
      "[Batch #113000/122727] Loss: 1364.0277407169342\n",
      "[Batch #113500/122727] Loss: 1366.0693571567535\n",
      "[Batch #114000/122727] Loss: 1353.140372991562\n",
      "[Batch #114500/122727] Loss: 1364.959320783615\n",
      "[Batch #115000/122727] Loss: 1365.416766166687\n",
      "[Batch #115500/122727] Loss: 1364.1646602153778\n",
      "[Batch #116000/122727] Loss: 1363.982435464859\n",
      "[Batch #116500/122727] Loss: 1351.1003692150116\n",
      "[Batch #117000/122727] Loss: 1352.871321439743\n",
      "[Batch #117500/122727] Loss: 1367.535540819168\n",
      "[Batch #118000/122727] Loss: 1356.8437285423279\n",
      "[Batch #118500/122727] Loss: 1358.8058292865753\n",
      "[Batch #119000/122727] Loss: 1357.9751183986664\n",
      "[Batch #119500/122727] Loss: 1364.4192633628845\n",
      "[Batch #120000/122727] Loss: 1358.6413229703903\n",
      "[Batch #120500/122727] Loss: 1360.3463413715363\n",
      "[Batch #121000/122727] Loss: 1355.0879037380219\n",
      "[Batch #121500/122727] Loss: 1357.2986507415771\n",
      "[Batch #122000/122727] Loss: 1357.903380870819\n",
      "[Batch #122500/122727] Loss: 1360.027718782425\n",
      "\n",
      "Validating model...\n",
      "Epoch 7 complete. Training loss: 2.7200592703255784 - Validation loss: 2.4982665493464307\n",
      "----- END OF EPOCH 7 -----\n",
      "\n",
      "----- STARTING EPOCH 8 -----\n",
      "Training model...\n",
      "[Batch #500/122727] Loss: 1366.0741965770721\n",
      "[Batch #1000/122727] Loss: 1351.042504787445\n",
      "[Batch #1500/122727] Loss: 1358.3496406078339\n",
      "[Batch #2000/122727] Loss: 1363.8135101795197\n",
      "[Batch #2500/122727] Loss: 1367.2959651947021\n",
      "[Batch #3000/122727] Loss: 1361.2782220840454\n",
      "[Batch #3500/122727] Loss: 1364.178394794464\n",
      "[Batch #4000/122727] Loss: 1361.2457743883133\n",
      "[Batch #4500/122727] Loss: 1364.71932387352\n",
      "[Batch #5000/122727] Loss: 1364.4818221330643\n",
      "[Batch #5500/122727] Loss: 1354.8812811374664\n",
      "[Batch #6000/122727] Loss: 1362.95057117939\n",
      "[Batch #6500/122727] Loss: 1357.311184644699\n",
      "[Batch #7000/122727] Loss: 1359.548008441925\n",
      "[Batch #7500/122727] Loss: 1351.631956577301\n",
      "[Batch #8000/122727] Loss: 1362.1864750385284\n",
      "[Batch #8500/122727] Loss: 1366.0589323043823\n",
      "[Batch #9000/122727] Loss: 1355.6339823007584\n",
      "[Batch #9500/122727] Loss: 1359.744132757187\n",
      "[Batch #10000/122727] Loss: 1357.7078168392181\n",
      "[Batch #10500/122727] Loss: 1362.5286889076233\n",
      "[Batch #11000/122727] Loss: 1361.4587738513947\n",
      "[Batch #11500/122727] Loss: 1356.630483865738\n",
      "[Batch #12000/122727] Loss: 1350.7553651332855\n",
      "[Batch #12500/122727] Loss: 1354.8017032146454\n",
      "[Batch #13000/122727] Loss: 1355.8540043830872\n",
      "[Batch #13500/122727] Loss: 1357.6802451610565\n",
      "[Batch #14000/122727] Loss: 1363.175849199295\n",
      "[Batch #14500/122727] Loss: 1356.4862990379333\n",
      "[Batch #15000/122727] Loss: 1359.0422270298004\n",
      "[Batch #15500/122727] Loss: 1363.1962459087372\n",
      "[Batch #16000/122727] Loss: 1370.3654737472534\n",
      "[Batch #16500/122727] Loss: 1351.0281212329865\n",
      "[Batch #17000/122727] Loss: 1353.108645915985\n",
      "[Batch #17500/122727] Loss: 1364.1383562088013\n",
      "[Batch #18000/122727] Loss: 1359.4505050182343\n",
      "[Batch #18500/122727] Loss: 1359.295087337494\n",
      "[Batch #19000/122727] Loss: 1357.4375839233398\n",
      "[Batch #19500/122727] Loss: 1363.8002237081528\n",
      "[Batch #20000/122727] Loss: 1355.5884063243866\n",
      "[Batch #20500/122727] Loss: 1361.2541222572327\n",
      "[Batch #21000/122727] Loss: 1360.5878415107727\n",
      "[Batch #21500/122727] Loss: 1359.444366812706\n",
      "[Batch #22000/122727] Loss: 1355.933052778244\n",
      "[Batch #22500/122727] Loss: 1363.4834952354431\n",
      "[Batch #23000/122727] Loss: 1352.2805457115173\n",
      "[Batch #23500/122727] Loss: 1354.3946093320847\n",
      "[Batch #24000/122727] Loss: 1356.0181052684784\n",
      "[Batch #24500/122727] Loss: 1360.2273547649384\n",
      "[Batch #25000/122727] Loss: 1359.3138761520386\n",
      "[Batch #25500/122727] Loss: 1371.9673173427582\n",
      "[Batch #26000/122727] Loss: 1360.1377079486847\n",
      "[Batch #26500/122727] Loss: 1365.2811031341553\n",
      "[Batch #27000/122727] Loss: 1352.6240191459656\n",
      "[Batch #27500/122727] Loss: 1372.9328513145447\n",
      "[Batch #28000/122727] Loss: 1357.7704393863678\n",
      "[Batch #28500/122727] Loss: 1362.2345089912415\n",
      "[Batch #29000/122727] Loss: 1358.082932472229\n",
      "[Batch #29500/122727] Loss: 1356.3426060676575\n",
      "[Batch #30000/122727] Loss: 1357.1626987457275\n",
      "[Batch #30500/122727] Loss: 1367.683861732483\n",
      "[Batch #31000/122727] Loss: 1363.7683329582214\n",
      "[Batch #31500/122727] Loss: 1358.4616284370422\n",
      "[Batch #32000/122727] Loss: 1350.171537399292\n",
      "[Batch #32500/122727] Loss: 1357.4013863801956\n",
      "[Batch #33000/122727] Loss: 1363.964069366455\n",
      "[Batch #33500/122727] Loss: 1356.7642986774445\n",
      "[Batch #34000/122727] Loss: 1356.4209151268005\n",
      "[Batch #34500/122727] Loss: 1367.4140532016754\n",
      "[Batch #35000/122727] Loss: 1362.3636593818665\n",
      "[Batch #35500/122727] Loss: 1357.2587838172913\n",
      "[Batch #36000/122727] Loss: 1350.2375597953796\n",
      "[Batch #36500/122727] Loss: 1353.2425661087036\n",
      "[Batch #37000/122727] Loss: 1357.4957246780396\n",
      "[Batch #37500/122727] Loss: 1353.1612372398376\n",
      "[Batch #38000/122727] Loss: 1353.3119814395905\n",
      "[Batch #38500/122727] Loss: 1347.5630300045013\n",
      "[Batch #39000/122727] Loss: 1366.0457401275635\n",
      "[Batch #39500/122727] Loss: 1351.757996082306\n",
      "[Batch #40000/122727] Loss: 1359.8432040214539\n",
      "[Batch #40500/122727] Loss: 1357.2951519489288\n",
      "[Batch #41000/122727] Loss: 1356.5491478443146\n",
      "[Batch #41500/122727] Loss: 1364.5416705608368\n",
      "[Batch #42000/122727] Loss: 1365.183298587799\n",
      "[Batch #42500/122727] Loss: 1362.2278192043304\n",
      "[Batch #43000/122727] Loss: 1359.0931193828583\n",
      "[Batch #43500/122727] Loss: 1351.730946779251\n",
      "[Batch #44000/122727] Loss: 1367.720327615738\n",
      "[Batch #44500/122727] Loss: 1364.9150211811066\n",
      "[Batch #45000/122727] Loss: 1343.6424894332886\n",
      "[Batch #45500/122727] Loss: 1357.41353225708\n",
      "[Batch #46000/122727] Loss: 1347.575520992279\n",
      "[Batch #46500/122727] Loss: 1356.8731355667114\n",
      "[Batch #47000/122727] Loss: 1359.9555237293243\n",
      "[Batch #47500/122727] Loss: 1352.480707168579\n",
      "[Batch #48000/122727] Loss: 1361.308405637741\n",
      "[Batch #48500/122727] Loss: 1361.5138700008392\n",
      "[Batch #49000/122727] Loss: 1356.4919917583466\n",
      "[Batch #49500/122727] Loss: 1359.483547091484\n",
      "[Batch #50000/122727] Loss: 1354.6049463748932\n",
      "[Batch #50500/122727] Loss: 1360.0476009845734\n",
      "[Batch #51000/122727] Loss: 1367.8833203315735\n",
      "[Batch #51500/122727] Loss: 1370.2613579034805\n",
      "[Batch #52000/122727] Loss: 1355.2898960113525\n",
      "[Batch #52500/122727] Loss: 1354.9583263397217\n",
      "[Batch #53000/122727] Loss: 1367.752917766571\n",
      "[Batch #53500/122727] Loss: 1354.5889110565186\n",
      "[Batch #54000/122727] Loss: 1360.4734077453613\n",
      "[Batch #54500/122727] Loss: 1356.9811948537827\n",
      "[Batch #55000/122727] Loss: 1360.2247219085693\n",
      "[Batch #55500/122727] Loss: 1364.23291015625\n",
      "[Batch #56000/122727] Loss: 1365.174569606781\n",
      "[Batch #56500/122727] Loss: 1360.0857589244843\n",
      "[Batch #57000/122727] Loss: 1364.9328434467316\n",
      "[Batch #57500/122727] Loss: 1363.0945637226105\n",
      "[Batch #58000/122727] Loss: 1368.8945310115814\n",
      "[Batch #58500/122727] Loss: 1362.3077228069305\n",
      "[Batch #59000/122727] Loss: 1367.1988065242767\n",
      "[Batch #59500/122727] Loss: 1358.0410525798798\n",
      "[Batch #60000/122727] Loss: 1353.532853603363\n",
      "[Batch #60500/122727] Loss: 1360.355110168457\n",
      "[Batch #61000/122727] Loss: 1367.153638124466\n",
      "[Batch #61500/122727] Loss: 1356.6817293167114\n",
      "[Batch #62000/122727] Loss: 1360.2929623126984\n",
      "[Batch #62500/122727] Loss: 1361.8791060447693\n",
      "[Batch #63000/122727] Loss: 1360.4130244255066\n",
      "[Batch #63500/122727] Loss: 1357.6557406187057\n",
      "[Batch #64000/122727] Loss: 1366.7677518129349\n",
      "[Batch #64500/122727] Loss: 1359.585898399353\n",
      "[Batch #65000/122727] Loss: 1349.4982340335846\n",
      "[Batch #65500/122727] Loss: 1359.0507600307465\n",
      "[Batch #66000/122727] Loss: 1355.524070262909\n",
      "[Batch #66500/122727] Loss: 1366.7574911117554\n",
      "[Batch #67000/122727] Loss: 1355.5663063526154\n",
      "[Batch #67500/122727] Loss: 1370.0012817382812\n",
      "[Batch #68000/122727] Loss: 1358.4402797222137\n",
      "[Batch #68500/122727] Loss: 1362.618328332901\n",
      "[Batch #69000/122727] Loss: 1367.7083995342255\n",
      "[Batch #69500/122727] Loss: 1366.0814549922943\n",
      "[Batch #70000/122727] Loss: 1366.107502937317\n",
      "[Batch #70500/122727] Loss: 1359.3473842144012\n",
      "[Batch #71000/122727] Loss: 1366.1801953315735\n",
      "[Batch #71500/122727] Loss: 1357.8955309391022\n",
      "[Batch #72000/122727] Loss: 1356.153459072113\n",
      "[Batch #72500/122727] Loss: 1353.0925295352936\n",
      "[Batch #73000/122727] Loss: 1367.9931902885437\n",
      "[Batch #73500/122727] Loss: 1358.7465143203735\n",
      "[Batch #74000/122727] Loss: 1358.782653093338\n",
      "[Batch #74500/122727] Loss: 1358.055641412735\n",
      "[Batch #75000/122727] Loss: 1364.1742067337036\n",
      "[Batch #75500/122727] Loss: 1360.2722569704056\n",
      "[Batch #76000/122727] Loss: 1352.226284980774\n",
      "[Batch #76500/122727] Loss: 1362.2827596664429\n",
      "[Batch #77000/122727] Loss: 1367.2999563217163\n",
      "[Batch #77500/122727] Loss: 1356.2330076694489\n",
      "[Batch #78000/122727] Loss: 1360.2045879364014\n",
      "[Batch #78500/122727] Loss: 1359.17542719841\n",
      "[Batch #79000/122727] Loss: 1362.8118188381195\n",
      "[Batch #79500/122727] Loss: 1361.3449285030365\n",
      "[Batch #80000/122727] Loss: 1363.7612637281418\n",
      "[Batch #80500/122727] Loss: 1354.3420476913452\n",
      "[Batch #81000/122727] Loss: 1360.1257998943329\n",
      "[Batch #81500/122727] Loss: 1377.0932545661926\n",
      "[Batch #82000/122727] Loss: 1359.4323737621307\n",
      "[Batch #82500/122727] Loss: 1376.61461186409\n",
      "[Batch #83000/122727] Loss: 1358.8530900478363\n",
      "[Batch #83500/122727] Loss: 1364.2203705310822\n",
      "[Batch #84000/122727] Loss: 1360.8925831317902\n",
      "[Batch #84500/122727] Loss: 1356.6352145671844\n",
      "[Batch #85000/122727] Loss: 1357.5546584129333\n",
      "[Batch #85500/122727] Loss: 1364.297779560089\n",
      "[Batch #86000/122727] Loss: 1364.8953516483307\n",
      "[Batch #86500/122727] Loss: 1369.3566417694092\n",
      "[Batch #87000/122727] Loss: 1356.2143368721008\n",
      "[Batch #87500/122727] Loss: 1361.0407509803772\n",
      "[Batch #88000/122727] Loss: 1364.9852211475372\n",
      "[Batch #88500/122727] Loss: 1365.6102213859558\n",
      "[Batch #89000/122727] Loss: 1361.3494448661804\n",
      "[Batch #89500/122727] Loss: 1364.2854702472687\n",
      "[Batch #90000/122727] Loss: 1367.5950491428375\n",
      "[Batch #90500/122727] Loss: 1359.6967415809631\n",
      "[Batch #91000/122727] Loss: 1349.2865886688232\n",
      "[Batch #91500/122727] Loss: 1358.6783454418182\n",
      "[Batch #92000/122727] Loss: 1354.0496904850006\n",
      "[Batch #92500/122727] Loss: 1356.4143867492676\n",
      "[Batch #93000/122727] Loss: 1355.2469010353088\n",
      "[Batch #93500/122727] Loss: 1361.5723359584808\n",
      "[Batch #94000/122727] Loss: 1365.1588571071625\n",
      "[Batch #94500/122727] Loss: 1361.8337805271149\n",
      "[Batch #95000/122727] Loss: 1369.3479237556458\n",
      "[Batch #95500/122727] Loss: 1358.6954292058945\n",
      "[Batch #96000/122727] Loss: 1356.5285873413086\n",
      "[Batch #96500/122727] Loss: 1363.338012933731\n",
      "[Batch #97000/122727] Loss: 1362.9375507831573\n",
      "[Batch #97500/122727] Loss: 1352.9662038087845\n",
      "[Batch #98000/122727] Loss: 1356.735199689865\n",
      "[Batch #98500/122727] Loss: 1359.8990992307663\n",
      "[Batch #99000/122727] Loss: 1362.0329840183258\n",
      "[Batch #99500/122727] Loss: 1362.3588211536407\n",
      "[Batch #100000/122727] Loss: 1357.0336589813232\n",
      "[Batch #100500/122727] Loss: 1359.7571196556091\n",
      "[Batch #101000/122727] Loss: 1365.7491202354431\n",
      "[Batch #101500/122727] Loss: 1359.349417924881\n",
      "[Batch #102000/122727] Loss: 1357.3268421888351\n",
      "[Batch #102500/122727] Loss: 1355.697295665741\n",
      "[Batch #103000/122727] Loss: 1360.3147764205933\n",
      "[Batch #103500/122727] Loss: 1365.4924566745758\n",
      "[Batch #104000/122727] Loss: 1364.6029953956604\n",
      "[Batch #104500/122727] Loss: 1356.9942014217377\n",
      "[Batch #105000/122727] Loss: 1348.0131213665009\n",
      "[Batch #105500/122727] Loss: 1362.5509505271912\n",
      "[Batch #106000/122727] Loss: 1366.140555858612\n",
      "[Batch #106500/122727] Loss: 1360.6497130393982\n",
      "[Batch #107000/122727] Loss: 1359.092086315155\n",
      "[Batch #107500/122727] Loss: 1356.383995771408\n",
      "[Batch #108000/122727] Loss: 1355.1648170948029\n",
      "[Batch #108500/122727] Loss: 1364.4145488739014\n",
      "[Batch #109000/122727] Loss: 1360.4149067401886\n",
      "[Batch #109500/122727] Loss: 1360.5341999530792\n",
      "[Batch #110000/122727] Loss: 1361.5787603855133\n",
      "[Batch #110500/122727] Loss: 1352.5069839954376\n",
      "[Batch #111000/122727] Loss: 1362.7544111013412\n",
      "[Batch #111500/122727] Loss: 1362.256523013115\n",
      "[Batch #112000/122727] Loss: 1362.4895548820496\n",
      "[Batch #112500/122727] Loss: 1358.86674118042\n",
      "[Batch #113000/122727] Loss: 1355.3074553012848\n",
      "[Batch #113500/122727] Loss: 1357.6882772445679\n",
      "[Batch #114000/122727] Loss: 1357.1860122680664\n",
      "[Batch #114500/122727] Loss: 1357.4860467910767\n",
      "[Batch #115000/122727] Loss: 1350.1567578315735\n",
      "[Batch #115500/122727] Loss: 1366.5101823806763\n",
      "[Batch #116000/122727] Loss: 1354.5767197608948\n",
      "[Batch #116500/122727] Loss: 1365.064206123352\n",
      "[Batch #117000/122727] Loss: 1364.1513049602509\n",
      "[Batch #117500/122727] Loss: 1355.355262517929\n",
      "[Batch #118000/122727] Loss: 1366.9793155193329\n",
      "[Batch #118500/122727] Loss: 1362.3281791210175\n",
      "[Batch #119000/122727] Loss: 1356.9375859498978\n",
      "[Batch #119500/122727] Loss: 1356.4901554584503\n",
      "[Batch #120000/122727] Loss: 1360.311770439148\n",
      "[Batch #120500/122727] Loss: 1354.1079332828522\n",
      "[Batch #121000/122727] Loss: 1352.1824913024902\n",
      "[Batch #121500/122727] Loss: 1358.472098350525\n",
      "[Batch #122000/122727] Loss: 1358.0414035320282\n",
      "[Batch #122500/122727] Loss: 1364.5665981769562\n",
      "\n",
      "Validating model...\n",
      "Epoch 8 complete. Training loss: 2.719947451845881 - Validation loss: 2.5100022413394236\n",
      "----- END OF EPOCH 8 -----\n",
      "\n",
      "Early stopping. Patience limit reached.\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    ds = torch.load(training_dataset, weights_only=False)\n",
    "    print(\"Preparing to train on\", len(ds), \"samples\")\n",
    "\n",
    "    batch_size = 64\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    print(\"DataLoader ready with\", len(dl), \"batches\")\n",
    "    \n",
    "    vds = torch.load(validation_dataset, weights_only=False)\n",
    "    vdl = DataLoader(vds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Performing training using {device}\")\n",
    "\n",
    "    model = ChessModel().to(device)\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    \n",
    "    print(\"Model will be saved to\", os.path.abspath(model_path))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 60;\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_loss = 2.4864 #float(\"inf\")\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"----- STARTING EPOCH\", epoch + 1, \"-----\")\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        model.train()\n",
    "        batch_count = 0\n",
    "        training_loss = 0\n",
    "        batch_checkpoint_loss = 0\n",
    "        for batch in dl:\n",
    "            model_input = batch[\"board\"].to(device)\n",
    "            best_moves = batch[\"next_move\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            logits = model(model_input)\n",
    "            loss = loss_fn(logits, best_moves)\n",
    "            training_loss += loss.item()\n",
    "            batch_checkpoint_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch_count+1) % 500 == 0:\n",
    "                print(f\"[Batch #{batch_count + 1}/{len(dl)}] Loss: {batch_checkpoint_loss}\")\n",
    "                batch_checkpoint_loss = 0\n",
    "            batch_count += 1\n",
    "        \n",
    "        training_loss /= len(dl)\n",
    "        \n",
    "        print(\"\\nValidating model...\")\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in vdl:\n",
    "                model_input = batch[\"board\"].to(device)\n",
    "                best_moves = batch[\"next_move\"].to(device)\n",
    "                logits = model(model_input)\n",
    "                loss = loss_fn(logits, best_moves)\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "        validation_loss /= len(vdl)\n",
    "        print(f\"Epoch {epoch + 1} complete. Training loss: {training_loss} - Validation loss: {validation_loss}\")\n",
    "        print(\"----- END OF EPOCH\", epoch + 1, \"-----\\n\")\n",
    "\n",
    "        if validation_loss < best_loss:\n",
    "            print(\"New best model found. Saving...\")\n",
    "            best_loss = validation_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping. Patience limit reached.\")\n",
    "                break\n",
    "            \n",
    "def train_value():\n",
    " \n",
    "    ds = torch.load(training_dataset, weights_only=False)\n",
    "    print(\"Preparing to train on\", len(ds), \"samples\")\n",
    "\n",
    "    batch_size = 64\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "    print(\"DataLoader ready with\", len(dl), \"batches\")\n",
    "    \n",
    "    vds = torch.load(validation_dataset, weights_only=False)\n",
    "    vdl = DataLoader(vds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = (\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\"\n",
    "        if torch.backends.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    print(f\"Performing training using {device}\")\n",
    "\n",
    "    model = ChessValueModel().to(device)\n",
    "    if os.path.exists(model_path):\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    \n",
    "    print(\"Model will be saved to\", os.path.abspath(model_path))\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    epochs = 50;\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(\"----- STARTING EPOCH\", epoch + 1, \"-----\")\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        model.train()\n",
    "        batch_count = 0\n",
    "        training_loss = 0\n",
    "        batch_checkpoint_loss = 0\n",
    "        for batch in dl:\n",
    "            model_input = batch[\"board\"].to(device)\n",
    "            results = batch[\"result\"].to(device)\n",
    "            results = results.unsqueeze(1).float()\n",
    "        \n",
    "            optimizer.zero_grad() \n",
    "            logits = model(model_input)\n",
    "              \n",
    "            loss = loss_fn(logits, results)\n",
    "            training_loss += loss.item()\n",
    "            batch_checkpoint_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch_count+1) % 500 == 0:\n",
    "                print(f\"[Batch #{batch_count + 1}/{len(dl)}] Loss: {batch_checkpoint_loss}\")\n",
    "                batch_checkpoint_loss = 0\n",
    "            batch_count += 1\n",
    "        \n",
    "        training_loss /= len(dl)\n",
    "        \n",
    "        print(\"\\nValidating model...\")\n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in vdl:\n",
    "                model_input = batch[\"board\"].to(device)\n",
    "                results = batch[\"result\"].to(device)\n",
    "                logits = model(model_input)\n",
    "                loss = loss_fn(logits, results.unsqueeze(1).float())\n",
    "                validation_loss += loss.item()\n",
    "                \n",
    "        validation_loss /= len(vdl)\n",
    "        print(f\"Epoch {epoch + 1} complete. Training loss: {training_loss} - Validation loss: {validation_loss}\")\n",
    "        print(\"----- END OF EPOCH\", epoch + 1, \"-----\\n\")\n",
    "\n",
    "        if validation_loss < best_loss:\n",
    "            print(\"New best model found. Saving...\")\n",
    "            best_loss = validation_loss\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping. Patience limit reached.\")\n",
    "                break\n",
    "\n",
    "    \n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
